{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e939f162",
   "metadata": {},
   "source": [
    "# Bi-LSTM GNSS Spoofing Detection\n",
    "\n",
    "A complete workflow for GNSS spoofing detection using a Bi-LSTM model in PyTorch. This notebook covers data loading, preprocessing, feature engineering, sequence creation, model training, evaluation, and export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc2abf",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "\n",
    "Import required libraries for GNSS spoofing detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fe31c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu126\n",
      "GPU Available: True\n",
      "GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Numpy version: 1.26.0\n",
      "Pandas version: 2.3.3\n",
      "All packages loaded successfully\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(\"All packages loaded successfully\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80439d",
   "metadata": {},
   "source": [
    "## 2. GNSS Data Loading\n",
    "\n",
    "Load the GNSS dataset from CSV or parquet files. Display basic statistics and sample rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b06b89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNSS data directory exists: True\n",
      "Found 240 observation JSON files\n",
      "Found 240 pvtSolution JSON files\n",
      "Found 240 satelliteInfomation JSON files\n",
      "Sample observation JSON keys: ['recordTime', 'VSG', 'VSE', 'VSB', 'VSQ', 'VSR', 'prMes_G1', 'doMes_G1', 'cpMes_G1', 'cn0_G1', 'prStd_G1', 'cpStd_G1', 'doStd_G1', 'prMes_G2', 'doMes_G2', 'cpMes_G2', 'cn0_G2', 'prStd_G2', 'cpStd_G2', 'doStd_G2', 'prMes_E1', 'doMes_E1', 'cpMes_E1', 'cn0_E1', 'prStd_E1', 'cpStd_E1', 'doStd_E1', 'prMes_E2', 'doMes_E2', 'cpMes_E2', 'cn0_E2', 'prStd_E2', 'cpStd_E2', 'doStd_E2', 'prMes_B1', 'doMes_B1', 'cpMes_B1', 'cn0_B1', 'prStd_B1', 'cpStd_B1', 'doStd_B1', 'prMes_B2', 'doMes_B2', 'cpMes_B2', 'cn0_B2', 'prStd_B2', 'cpStd_B2', 'doStd_B2', 'prMes_Q1', 'doMes_Q1', 'cpMes_Q1', 'cn0_Q1', 'prStd_Q1', 'cpStd_Q1', 'doStd_Q1', 'prMes_Q2', 'doMes_Q2', 'cpMes_Q2', 'cn0_Q2', 'prStd_Q2', 'cpStd_Q2', 'doStd_Q2', 'prMes_R1', 'doMes_R1', 'cpMes_R1', 'cn0_R1', 'prStd_R1', 'cpStd_R1', 'doStd_R1', 'prMes_R2', 'doMes_R2', 'cpMes_R2', 'cn0_R2', 'prStd_R2', 'cpStd_R2', 'doStd_R2']\n",
      "Sample pvtSolution JSON keys: ['recordTime', 'numSV', 'nano', 'lon', 'lat', 'height', 'velN', 'velE', 'velD', 'hMSL', 'hAcc', 'vAcc', 'sAcc', 'gSpeed', 'headMot', 'headAcc', 'ecefX', 'ecefY', 'ecefZ', 'clkB', 'clkD', 'tAcc', 'fAcc', 'gDOP', 'pDOP', 'tDOP', 'vDOP', 'hDOP', 'nDOP', 'eDOP']\n",
      "Sample satelliteInfomation JSON keys: ['recordTime', 'numSvs', 'svId_G', 'svUsed_G', 'cno_G', 'elev_G', 'azim_G', 'prRes_G', 'qualityInd_G', 'health_G', 'svId_E', 'svUsed_E', 'cno_E', 'elev_E', 'azim_E', 'prRes_E', 'qualityInd_E', 'health_E', 'svId_B', 'svUsed_B', 'cno_B', 'elev_B', 'azim_B', 'prRes_B', 'qualityInd_B', 'health_B', 'svId_Q', 'svUsed_Q', 'cno_Q', 'elev_Q', 'azim_Q', 'prRes_Q', 'qualityInd_Q', 'health_Q', 'svId_R', 'svUsed_R', 'cno_R', 'elev_R', 'azim_R', 'prRes_R', 'qualityInd_R', 'health_R']\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m obs_data = []\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m obs_files:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     data = \u001b[43mload_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m     45\u001b[39m         obs_data.append(data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mload_json_file\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_json_file\u001b[39m(filepath):\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\json\\__init__.py:293\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(fp, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    294\u001b[39m         \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28mcls\u001b[39m, object_hook=object_hook,\n\u001b[32m    295\u001b[39m         parse_float=parse_float, parse_int=parse_int,\n\u001b[32m    296\u001b[39m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# GNSS dataset configuration\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Update this path to your actual GNSS dataset directory\n",
    "gnss_data_dir = Path(\"./GNSS3/Processed data\")\n",
    "print(f\"GNSS data directory exists: {gnss_data_dir.exists()}\")\n",
    "\n",
    "obs_files = []\n",
    "pvt_files = []\n",
    "sat_files = []\n",
    "# List JSON files for observation, pvtSolution, satelliteInfomation\n",
    "for root, dirs, files in os.walk(gnss_data_dir):\n",
    "    for dir in dirs: # dirs 21...30\n",
    "        obs_files.extend(sorted(glob.glob(str(gnss_data_dir / dir / \"observation*.json\"))))\n",
    "        pvt_files.extend(sorted(glob.glob(str(gnss_data_dir / dir / \"pvtSolution*.json\"))))\n",
    "        sat_files.extend(sorted(glob.glob(str(gnss_data_dir / dir / \"satelliteInfomation*.json\"))))\n",
    "\n",
    "print(f\"Found {len(obs_files)} observation JSON files\")\n",
    "print(f\"Found {len(pvt_files)} pvtSolution JSON files\")\n",
    "print(f\"Found {len(sat_files)} satelliteInfomation JSON files\")\n",
    "\n",
    "# Load a sample of each type for inspection\n",
    "def load_json_file(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Inspect first file of each type\n",
    "sample_obs = load_json_file(obs_files[0]) if obs_files else None\n",
    "sample_pvt = load_json_file(pvt_files[0]) if pvt_files else None\n",
    "sample_sat = load_json_file(sat_files[0]) if sat_files else None\n",
    "\n",
    "print(\"Sample observation JSON keys:\", list(sample_obs.keys()) if sample_obs else \"None\")\n",
    "print(\"Sample pvtSolution JSON keys:\", list(sample_pvt.keys()) if sample_pvt else \"None\")\n",
    "print(\"Sample satelliteInfomation JSON keys:\", list(sample_sat.keys()) if sample_sat else \"None\")\n",
    "\n",
    "# Convert all observation JSONs to DataFrame (example for observation)\n",
    "obs_data = []\n",
    "for f in obs_files:\n",
    "    data = load_json_file(f)\n",
    "    if isinstance(data, dict):\n",
    "        obs_data.append(data)\n",
    "    elif isinstance(data, list):\n",
    "        obs_data.extend(data)\n",
    "gnss_df = pd.DataFrame(obs_data)\n",
    "print(\"GNSS DataFrame loaded from observation JSONs:\")\n",
    "print(f\"  Records: {len(gnss_df):,}\")\n",
    "print(f\"  Columns: {list(gnss_df.columns)}\")\n",
    "print(gnss_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bbcb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1a616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNSS DataFrame shape: (24, 77)\n",
      "GNSS DataFrame columns: ['recordTime', 'VSG', 'VSE', 'VSB', 'VSQ', 'VSR', 'prMes_G1', 'doMes_G1', 'cpMes_G1', 'cn0_G1', 'prStd_G1', 'cpStd_G1', 'doStd_G1', 'prMes_G2', 'doMes_G2', 'cpMes_G2', 'cn0_G2', 'prStd_G2', 'cpStd_G2', 'doStd_G2', 'prMes_E1', 'doMes_E1', 'cpMes_E1', 'cn0_E1', 'prStd_E1', 'cpStd_E1', 'doStd_E1', 'prMes_E2', 'doMes_E2', 'cpMes_E2', 'cn0_E2', 'prStd_E2', 'cpStd_E2', 'doStd_E2', 'prMes_B1', 'doMes_B1', 'cpMes_B1', 'cn0_B1', 'prStd_B1', 'cpStd_B1', 'doStd_B1', 'prMes_B2', 'doMes_B2', 'cpMes_B2', 'cn0_B2', 'prStd_B2', 'cpStd_B2', 'doStd_B2', 'prMes_Q1', 'doMes_Q1', 'cpMes_Q1', 'cn0_Q1', 'prStd_Q1', 'cpStd_Q1', 'doStd_Q1', 'prMes_Q2', 'doMes_Q2', 'cpMes_Q2', 'cn0_Q2', 'prStd_Q2', 'cpStd_Q2', 'doStd_Q2', 'prMes_R1', 'doMes_R1', 'cpMes_R1', 'cn0_R1', 'prStd_R1', 'cpStd_R1', 'doStd_R1', 'prMes_R2', 'doMes_R2', 'cpMes_R2', 'cn0_R2', 'prStd_R2', 'cpStd_R2', 'doStd_R2', 'scenario']\n",
      "\n",
      "Head:\n"
     ]
    }
   ],
   "source": [
    "# --- GNSS Data EDA (Exploratory Data Analysis) ---\n",
    "print(f\"GNSS DataFrame shape: {gnss_df.shape}\")\n",
    "print(f\"GNSS DataFrame columns: {list(gnss_df.columns)}\")\n",
    "print(\"\\nHead:\")\n",
    "print(gnss_df.head())\n",
    "print(\"\\nDescribe:\")\n",
    "print(gnss_df.describe(include='all'))\n",
    "\n",
    "# Check for unique device or identifier columns\n",
    "for col in ['device_id', 'mmsi']:\n",
    "    if col in gnss_df.columns:\n",
    "        print(f\"Unique {col} values: {gnss_df[col].nunique()}\")\n",
    "        print(f\"{col} values: {gnss_df[col].unique()}\")\n",
    "\n",
    "# Check timestamp column\n",
    "if 'timestamp' in gnss_df.columns:\n",
    "    try:\n",
    "        gnss_df['datetime'] = pd.to_datetime(gnss_df['timestamp'], errors='coerce')\n",
    "        print(f\"\\nDate range: {gnss_df['datetime'].min()} to {gnss_df['datetime'].max()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting timestamp: {e}\")\n",
    "\n",
    "# Value counts and non-zero checks for key columns\n",
    "for col in ['speed', 'course', 'lat', 'lon']:\n",
    "    if col in gnss_df.columns:\n",
    "        print(f\"\\n{col} value counts (top 10):\")\n",
    "        print(gnss_df[col].value_counts().head(10))\n",
    "        print(f\"\\n{col} summary:\")\n",
    "        print(gnss_df[col].describe())\n",
    "        if gnss_df[col].dtype in [np.float64, np.int64]:\n",
    "            print(f\"Non-zero {col}:\", (gnss_df[col] != 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d4aef",
   "metadata": {},
   "source": [
    "Label the data with attack type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dea7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: label by file path\n",
    "def get_label_from_filepath(filepath):\n",
    "    if \"Spoofing\" in str(filepath):\n",
    "        return \"spoofing\"\n",
    "    elif \"Jamming\" in str(filepath):\n",
    "        return \"jamming\"\n",
    "    else:\n",
    "        return \"clean\"\n",
    "\n",
    "# Apply labels to the DataFrame\n",
    "gnss_df[\"scenario\"] = [get_label_from_filepath(f) for f in obs_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "for day_folder in range(21, 31):  # days 21 to 30\n",
    "    path = f'processed/{day_folder}'\n",
    "    obs_files = glob.glob(f'{path}/observation*.json')\n",
    "    pvt_files = glob.glob(f'{path}/pvtSolution*.json')\n",
    "    sat_files = glob.glob(f'{path}/satelliteInformation*.json')\n",
    "\n",
    "    obs_data = [load_json_file(f) for f in obs_files]\n",
    "    pvt_data = [load_json_file(f) for f in pvt_files]\n",
    "    sat_data = [load_json_file(f) for f in sat_files]\n",
    "\n",
    "    obs_df = pd.DataFrame(obs_data)\n",
    "    pvt_df = pd.DataFrame(pvt_data)\n",
    "    sat_df = pd.DataFrame(sat_data)\n",
    "\n",
    "    merged = obs_df.merge(pvt_df, on=\"recordTime\", how=\"left\")\n",
    "    merged = merged.merge(sat_df, on=\"recordTime\", how=\"left\")\n",
    "\n",
    "    all_dfs.append(merged)\n",
    "\n",
    "full_df = pd.concat(all_dfs)\n",
    "full_df['recordTime'] = pd.to_datetime(full_df['recordTime'])\n",
    "full_df.set_index('recordTime', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a37791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recordTime\n",
      "<class 'list'>    24\n",
      "Name: count, dtype: int64\n",
      "recordTime\n",
      "<class 'list'>    24\n",
      "Name: count, dtype: int64\n",
      "recordTime\n",
      "<class 'list'>    24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(gnss_df['recordTime'].apply(type).value_counts())\n",
    "print(pvt_df['recordTime'].apply(type).value_counts())\n",
    "print(sat_df['recordTime'].apply(type).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee0d98",
   "metadata": {},
   "source": [
    "recordTime column in all three DataFrames is a list, so flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30921882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten recordTime lists to single values\n",
    "gnss_df['recordTime'] = gnss_df['recordTime'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "pvt_df['recordTime'] = pvt_df['recordTime'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "sat_df['recordTime'] = sat_df['recordTime'].apply(lambda x: x[0] if isinstance(x, list) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnss_df['recordTime'] = pd.to_datetime(gnss_df['recordTime'])\n",
    "pvt_df['recordTime'] = pd.to_datetime(pvt_df['recordTime'])\n",
    "sat_df['recordTime'] = pd.to_datetime(sat_df['recordTime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266026c",
   "metadata": {},
   "source": [
    "Merge with PVT or satellite info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3eb24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame columns: Index(['recordTime', 'VSG', 'VSE', 'VSB', 'VSQ', 'VSR', 'prMes_G1', 'doMes_G1',\n",
      "       'cpMes_G1', 'cn0_G1',\n",
      "       ...\n",
      "       'qualityInd_Q', 'health_Q', 'svId_R', 'svUsed_R', 'cno_R', 'elev_R',\n",
      "       'azim_R', 'prRes_R', 'qualityInd_R', 'health_R'],\n",
      "      dtype='object', length=147)\n",
      "Records after merge: 24\n"
     ]
    }
   ],
   "source": [
    "# Load PVT and Satellite info DataFrames (similar to obs_data)\n",
    "pvt_data = [load_json_file(f) for f in pvt_files]\n",
    "pvt_df = pd.DataFrame(pvt_data)\n",
    "\n",
    "sat_data = [load_json_file(f) for f in sat_files]\n",
    "sat_df = pd.DataFrame(sat_data)\n",
    "\n",
    "# Make sure recordTime is scalar (flatten lists if needed)\n",
    "gnss_df['recordTime'] = gnss_df['recordTime'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "pvt_df['recordTime'] = pvt_df['recordTime'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "sat_df['recordTime'] = sat_df['recordTime'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "# Optional: convert to datetime for consistent merging\n",
    "gnss_df['recordTime'] = pd.to_datetime(gnss_df['recordTime'])\n",
    "pvt_df['recordTime'] = pd.to_datetime(pvt_df['recordTime'])\n",
    "sat_df['recordTime'] = pd.to_datetime(sat_df['recordTime'])\n",
    "\n",
    "# Merge with GNSS observation by timestamp\n",
    "merged_df = gnss_df.merge(pvt_df, on=\"recordTime\", how=\"left\")\n",
    "merged_df = merged_df.merge(sat_df, on=\"recordTime\", how=\"left\")\n",
    "\n",
    "# Inspect merged DataFrame\n",
    "print(\"Merged DataFrame columns:\", merged_df.columns)\n",
    "print(f\"Records after merge: {len(merged_df):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314bb638",
   "metadata": {},
   "source": [
    "Since merged_df comes from GNSS, PVT, and satellite JSONs, likely we have columns where missing values mean “data not available” rather than “carry the last value forward.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca705715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "missing_summary = merged_df.isna().sum().sort_values(ascending=False)\n",
    "print(missing_summary[missing_summary > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Forward fill / backward fill only numeric columns\n",
    "# numeric_cols = merged_df.select_dtypes(include='number').columns\n",
    "# merged_df[numeric_cols] = merged_df[numeric_cols].fillna(method='ffill')\n",
    "# merged_df[numeric_cols] = merged_df[numeric_cols].fillna(method='bfill')\n",
    "\n",
    "# # For object or list columns, you may choose to fill with a placeholder\n",
    "# object_cols = merged_df.select_dtypes(include='object').columns\n",
    "# merged_df[object_cols] = merged_df[object_cols].fillna('missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e636bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_summary = merged_df.isna().sum().sort_values(ascending=False)\n",
    "# print(missing_summary[missing_summary > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f1b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recordTime      datetime64[ns]\n",
      "VSG                     object\n",
      "VSE                     object\n",
      "VSB                     object\n",
      "VSQ                     object\n",
      "                     ...      \n",
      "elev_R                  object\n",
      "azim_R                  object\n",
      "prRes_R                 object\n",
      "qualityInd_R            object\n",
      "health_R                object\n",
      "Length: 147, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.dtypes)\n",
    "merged_df['recordTime'] = pd.to_datetime(merged_df['recordTime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fecd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate recordTime entries: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate timestamps\n",
    "duplicate_count = merged_df.duplicated(subset=['recordTime']).sum()\n",
    "print(f\"Number of duplicate recordTime entries: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca8881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2023-09-30 00:00:00\n",
      "1   2023-09-30 01:00:00\n",
      "2   2023-09-30 10:00:00\n",
      "3   2023-09-30 11:00:00\n",
      "4   2023-09-30 12:00:00\n",
      "Name: recordTime, dtype: datetime64[ns]\n",
      "recordTime\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>    24\n",
      "Name: count, dtype: int64\n",
      "Number of invalid timestamps: 0\n"
     ]
    }
   ],
   "source": [
    "# Check first few entries\n",
    "print(merged_df['recordTime'].head())\n",
    "\n",
    "# Check the type of each entry\n",
    "print(merged_df['recordTime'].map(type).value_counts())\n",
    "\n",
    "# Optional: try parsing with errors='coerce' to see if any fail\n",
    "invalid_times = pd.to_datetime(merged_df['recordTime'], errors='coerce').isna().sum()\n",
    "print(f\"Number of invalid timestamps: {invalid_times}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7710b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.set_index('recordTime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd23abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2023-09-30 00:00:00', '2023-09-30 01:00:00',\n",
      "               '2023-09-30 10:00:00', '2023-09-30 11:00:00',\n",
      "               '2023-09-30 12:00:00', '2023-09-30 13:00:00',\n",
      "               '2023-09-30 14:00:00', '2023-09-30 15:00:00',\n",
      "               '2023-09-30 16:00:00', '2023-09-30 17:00:00',\n",
      "               '2023-09-30 18:00:00', '2023-09-30 19:00:00',\n",
      "               '2023-09-30 02:00:00', '2023-09-30 20:00:00',\n",
      "               '2023-09-30 21:00:00', '2023-09-30 22:00:00',\n",
      "               '2023-09-30 23:00:00', '2023-09-30 03:00:00',\n",
      "               '2023-09-30 04:00:00', '2023-09-30 05:00:00',\n",
      "               '2023-09-30 06:00:00', '2023-09-30 07:00:00',\n",
      "               '2023-09-30 08:00:00', '2023-09-30 09:00:00'],\n",
      "              dtype='datetime64[ns]', name='recordTime', freq=None)\n",
      "recordTime\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.index)       # shows the current index object\n",
    "print(merged_df.index.name)  # name of the index column, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdaaa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 24 entries, 2023-09-30 00:00:00 to 2023-09-30 09:00:00\n",
      "Columns: 146 entries, VSG to health_R\n",
      "dtypes: object(146)\n",
      "memory usage: 27.6+ KB\n",
      "None\n",
      "                                                      VSG  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 2.0, 0.0, 4.0, 0.0, 0.0, 7.0, 8.0, 0.0,...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                      VSE  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                      VSB  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 7.0, 0.0, 9.0,...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                      VSQ  \\\n",
      "count                                                  24   \n",
      "unique                                                  8   \n",
      "top     [[0.0, 2.0, 3.0, 4.0, 0.0, 0.0, 7.0, 0.0, 0.0,...   \n",
      "freq                                                   17   \n",
      "\n",
      "                                                      VSR  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                 prMes_G1  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 23946812.04589531, 0.0, 27476735.607809...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                 doMes_G1  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, -391.444091796875, 0.0, -3435.6875, 0.0...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                 cpMes_G1  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 125841541.47352295, 0.0, 144391219.7590...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                   cn0_G1  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 45.0, 0.0, 28.0, 0.0, 0.0, 36.0, 41.0, ...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                 prStd_G1  ...  \\\n",
      "count                                                  24  ...   \n",
      "unique                                                 24  ...   \n",
      "top     [[0.0, 3.0, 0.0, 13.0, 0.0, 0.0, 5.0, 4.0, 0.0...  ...   \n",
      "freq                                                    1  ...   \n",
      "\n",
      "                                             qualityInd_Q  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.11, 7.0, 7.0, 7.0, 0.11, 0.11, 7.0, 0.11, ...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                 health_Q  \\\n",
      "count                                                  24   \n",
      "unique                                                  3   \n",
      "top     [[0.11, 1.0, 1.0, 1.0, 0.11, 0.11, 1.0, 0.11, ...   \n",
      "freq                                                   22   \n",
      "\n",
      "                                                   svId_R  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 0.0, 3.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                 svUsed_R  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.11, 0.11, 0.0, 0.0, 0.11, 0.11, 0.11, 0.11...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                    cno_R  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 0.0, 17.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                   elev_R  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 0.0, 9.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                   azim_R  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 0.0, 254.0, 300.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                  prRes_R  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.0, 0.0, -171.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                             qualityInd_R  \\\n",
      "count                                                  24   \n",
      "unique                                                 24   \n",
      "top     [[0.11, 0.11, 4.0, 0.0, 0.11, 0.11, 0.11, 0.11...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                 health_R  \n",
      "count                                                  24  \n",
      "unique                                                 24  \n",
      "top     [[0.11, 0.11, 1.0, 1.0, 0.11, 0.11, 0.11, 0.11...  \n",
      "freq                                                    1  \n",
      "\n",
      "[4 rows x 146 columns]\n",
      "                                                                   VSG  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 2.0, 0.0, 4.0, 0.0, 0.0, 7.0, 8.0, 0.0,...   \n",
      "2023-09-30 01:00:00  [[0.0, 2.0, 3.0, 0.0, 0.0, 0.0, 7.0, 8.0, 0.0,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "\n",
      "                                                                   VSE  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 01:00:00  [[0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 8.0, 0.0,...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 0.0,...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 0.0,...   \n",
      "\n",
      "                                                                   VSB  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 7.0, 0.0, 9.0,...   \n",
      "2023-09-30 01:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 9.0,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 0.0, 8.0, 0.0,...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 0.0, 8.0, 0.0,...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 0.0, 8.0, 9.0,...   \n",
      "\n",
      "                                                                   VSQ  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 2.0, 3.0, 4.0, 0.0, 0.0, 7.0, 0.0, 0.0,...   \n",
      "2023-09-30 01:00:00  [[0.0, 2.0, 3.0, 4.0, 0.0, 0.0, 7.0, 0.0, 0.0,...   \n",
      "2023-09-30 10:00:00  [[0.0, 2.0, 3.0, 4.0, 0.0, 0.0, 7.0, 0.0, 0.0,...   \n",
      "2023-09-30 11:00:00  [[0.0, 2.0, 3.0, 4.0, 0.0, 0.0, 7.0, 0.0, 0.0,...   \n",
      "2023-09-30 12:00:00  [[0.0, 2.0, 3.0, 4.0, 0.0, 0.0, 7.0, 0.0, 0.0,...   \n",
      "\n",
      "                                                                   VSR  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 01:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 4.0, 5.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 7.0, 0.0, 0.0,...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 5.0, 6.0, 7.0, 0.0, 0.0,...   \n",
      "\n",
      "                                                              prMes_G1  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 23946812.04589531, 0.0, 27476735.607809...   \n",
      "2023-09-30 01:00:00  [[0.0, 24951496.741543498, 27612082.803135257,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 0.0, 20454542.375636898, 0.0,...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 21787090.954185072, 0.0,...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 23248005.59367111, 0.0, ...   \n",
      "\n",
      "                                                              doMes_G1  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, -391.444091796875, 0.0, -3435.6875, 0.0...   \n",
      "2023-09-30 01:00:00  [[0.0, -2443.0341796875, 1912.2646484375, 0.0,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 0.0, -1872.4385986328125, 0.0...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, -2010.6077880859375, 0.0...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, -2284.537841796875, 0.0,...   \n",
      "\n",
      "                                                              cpMes_G1  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 125841541.47352295, 0.0, 144391219.7590...   \n",
      "2023-09-30 01:00:00  [[0.0, 131121204.51238301, 145102473.70772988,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 0.0, 107489268.6149776, 0.0, ...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 114491815.27032278, 0.0,...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 122168868.47985142, 0.0,...   \n",
      "\n",
      "                                                                cn0_G1  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 45.0, 0.0, 28.0, 0.0, 0.0, 36.0, 41.0, ...   \n",
      "2023-09-30 01:00:00  [[0.0, 47.0, 24.0, 0.0, 0.0, 0.0, 0.0, 39.0, 0...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 0.0, 46.0, 0.0, 0.0, 0.0, 0.0...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 47.0, 0.0, 0.0, 0.0, 0.0...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 43.0, 0.0, 0.0, 0.0, 0.0...   \n",
      "\n",
      "                                                              prStd_G1  ...  \\\n",
      "recordTime                                                              ...   \n",
      "2023-09-30 00:00:00  [[0.0, 3.0, 0.0, 13.0, 0.0, 0.0, 5.0, 4.0, 0.0...  ...   \n",
      "2023-09-30 01:00:00  [[0.0, 3.0, 9.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0,...  ...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0,...  ...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0,...  ...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0,...  ...   \n",
      "\n",
      "                                                          qualityInd_Q  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.11, 7.0, 7.0, 7.0, 0.11, 0.11, 7.0, 0.11, ...   \n",
      "2023-09-30 01:00:00  [[0.11, 7.0, 7.0, 7.0, 0.11, 0.11, 7.0, 0.11, ...   \n",
      "2023-09-30 10:00:00  [[0.11, 7.0, 7.0, 7.0, 0.11, 0.11, 7.0, 0.11, ...   \n",
      "2023-09-30 11:00:00  [[0.11, 7.0, 7.0, 7.0, 0.11, 0.11, 7.0, 0.11, ...   \n",
      "2023-09-30 12:00:00  [[0.11, 1.0, 7.0, 7.0, 0.11, 0.11, 7.0, 0.11, ...   \n",
      "\n",
      "                                                              health_Q  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.11, 1.0, 1.0, 1.0, 0.11, 0.11, 1.0, 0.11, ...   \n",
      "2023-09-30 01:00:00  [[0.11, 1.0, 1.0, 1.0, 0.11, 0.11, 1.0, 0.11, ...   \n",
      "2023-09-30 10:00:00  [[0.11, 1.0, 1.0, 1.0, 0.11, 0.11, 1.0, 0.11, ...   \n",
      "2023-09-30 11:00:00  [[0.11, 1.0, 1.0, 1.0, 0.11, 0.11, 1.0, 0.11, ...   \n",
      "2023-09-30 12:00:00  [[0.11, 1.0, 1.0, 1.0, 0.11, 0.11, 1.0, 0.11, ...   \n",
      "\n",
      "                                                                svId_R  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 0.0, 3.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 01:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 4.0, 5.0, 6.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 5.0, 6.0, 7.0, 0.0, 0.0,...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 5.0, 6.0, 7.0, 0.0, 9.0,...   \n",
      "\n",
      "                                                              svUsed_R  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.11, 0.11, 0.0, 0.0, 0.11, 0.11, 0.11, 0.11...   \n",
      "2023-09-30 01:00:00  [[0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0....   \n",
      "2023-09-30 10:00:00  [[0.11, 0.11, 0.11, 1.0, 1.0, 0.0, 0.11, 0.11,...   \n",
      "2023-09-30 11:00:00  [[0.11, 0.11, 0.11, 0.11, 1.0, 0.0, 1.0, 0.11,...   \n",
      "2023-09-30 12:00:00  [[0.11, 0.11, 0.11, 0.11, 0.0, 1.0, 1.0, 0.11,...   \n",
      "\n",
      "                                                                 cno_R  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 0.0, 17.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
      "2023-09-30 01:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 46.0, 52.0, 0.0, 0.0, 0.0, 0....   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 52.0, 0.0, 27.0, 0.0, 0....   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 46.0, 33.0, 21.0, 0.0, 0...   \n",
      "\n",
      "                                                                elev_R  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 0.0, 9.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 01:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 12.0, 57.0, 41.0, 0.0, 0.0, 0...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 29.0, 51.0, 18.0, 0.0, 0...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 10.0, 45.0, 30.0, 0.0, 5...   \n",
      "\n",
      "                                                                azim_R  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 0.0, 254.0, 300.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2023-09-30 01:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, 30.0, 0.0, 244.0, 0.0, 0.0, 0...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 22.0, 295.0, 247.0, 0.0,...   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 44.0, 344.0, 280.0, 0.0,...   \n",
      "\n",
      "                                                               prRes_R  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.0, 0.0, -171.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0...   \n",
      "2023-09-30 01:00:00  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
      "2023-09-30 10:00:00  [[0.0, 0.0, 0.0, -2.4, 0.3, 0.0, 0.0, 0.0, 0.0...   \n",
      "2023-09-30 11:00:00  [[0.0, 0.0, 0.0, 0.0, 1.1, 0.0, -86.8, 0.0, 0....   \n",
      "2023-09-30 12:00:00  [[0.0, 0.0, 0.0, 0.0, 2.6, 4.9, -104.0, 0.0, 0...   \n",
      "\n",
      "                                                          qualityInd_R  \\\n",
      "recordTime                                                               \n",
      "2023-09-30 00:00:00  [[0.11, 0.11, 4.0, 0.0, 0.11, 0.11, 0.11, 0.11...   \n",
      "2023-09-30 01:00:00  [[0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0....   \n",
      "2023-09-30 10:00:00  [[0.11, 0.11, 0.11, 7.0, 7.0, 1.0, 0.11, 0.11,...   \n",
      "2023-09-30 11:00:00  [[0.11, 0.11, 0.11, 0.11, 7.0, 1.0, 7.0, 0.11,...   \n",
      "2023-09-30 12:00:00  [[0.11, 0.11, 0.11, 0.11, 7.0, 7.0, 4.0, 0.11,...   \n",
      "\n",
      "                                                              health_R  \n",
      "recordTime                                                              \n",
      "2023-09-30 00:00:00  [[0.11, 0.11, 1.0, 1.0, 0.11, 0.11, 0.11, 0.11...  \n",
      "2023-09-30 01:00:00  [[0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0....  \n",
      "2023-09-30 10:00:00  [[0.11, 0.11, 0.11, 1.0, 1.0, 1.0, 0.11, 0.11,...  \n",
      "2023-09-30 11:00:00  [[0.11, 0.11, 0.11, 0.11, 1.0, 1.0, 1.0, 0.11,...  \n",
      "2023-09-30 12:00:00  [[0.11, 0.11, 0.11, 0.11, 1.0, 1.0, 1.0, 0.11,...  \n",
      "\n",
      "[5 rows x 146 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.info())        # check column types, memory usage\n",
    "print(merged_df.describe())    # numeric stats\n",
    "print(merged_df.head())        # first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918c4b9",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "\n",
    "Apply domain-specific cleaning rules: remove invalid coordinates, handle missing values, and filter outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525be5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnss_df columns: ['recordTime', 'VSG', 'VSE', 'VSB', 'VSQ', 'VSR', 'prMes_G1', 'doMes_G1', 'cpMes_G1', 'cn0_G1', 'prStd_G1', 'cpStd_G1', 'doStd_G1', 'prMes_G2', 'doMes_G2', 'cpMes_G2', 'cn0_G2', 'prStd_G2', 'cpStd_G2', 'doStd_G2', 'prMes_E1', 'doMes_E1', 'cpMes_E1', 'cn0_E1', 'prStd_E1', 'cpStd_E1', 'doStd_E1', 'prMes_E2', 'doMes_E2', 'cpMes_E2', 'cn0_E2', 'prStd_E2', 'cpStd_E2', 'doStd_E2', 'prMes_B1', 'doMes_B1', 'cpMes_B1', 'cn0_B1', 'prStd_B1', 'cpStd_B1', 'doStd_B1', 'prMes_B2', 'doMes_B2', 'cpMes_B2', 'cn0_B2', 'prStd_B2', 'cpStd_B2', 'doStd_B2', 'prMes_Q1', 'doMes_Q1', 'cpMes_Q1', 'cn0_Q1', 'prStd_Q1', 'cpStd_Q1', 'doStd_Q1', 'prMes_Q2', 'doMes_Q2', 'cpMes_Q2', 'cn0_Q2', 'prStd_Q2', 'cpStd_Q2', 'doStd_Q2', 'prMes_R1', 'doMes_R1', 'cpMes_R1', 'cn0_R1', 'prStd_R1', 'cpStd_R1', 'doStd_R1', 'prMes_R2', 'doMes_R2', 'cpMes_R2', 'cn0_R2', 'prStd_R2', 'cpStd_R2', 'doStd_R2']\n"
     ]
    }
   ],
   "source": [
    "# Print columns of gnss_df to debug KeyError\n",
    "print(\"gnss_df columns:\", list(gnss_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7a782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clean_gnss_data] Columns: ['recordTime', 'VSG', 'VSE', 'VSB', 'VSQ', 'VSR', 'prMes_G1', 'doMes_G1', 'cpMes_G1', 'cn0_G1', 'prStd_G1', 'cpStd_G1', 'doStd_G1', 'prMes_G2', 'doMes_G2', 'cpMes_G2', 'cn0_G2', 'prStd_G2', 'cpStd_G2', 'doStd_G2', 'prMes_E1', 'doMes_E1', 'cpMes_E1', 'cn0_E1', 'prStd_E1', 'cpStd_E1', 'doStd_E1', 'prMes_E2', 'doMes_E2', 'cpMes_E2', 'cn0_E2', 'prStd_E2', 'cpStd_E2', 'doStd_E2', 'prMes_B1', 'doMes_B1', 'cpMes_B1', 'cn0_B1', 'prStd_B1', 'cpStd_B1', 'doStd_B1', 'prMes_B2', 'doMes_B2', 'cpMes_B2', 'cn0_B2', 'prStd_B2', 'cpStd_B2', 'doStd_B2', 'prMes_Q1', 'doMes_Q1', 'cpMes_Q1', 'cn0_Q1', 'prStd_Q1', 'cpStd_Q1', 'doStd_Q1', 'prMes_Q2', 'doMes_Q2', 'cpMes_Q2', 'cn0_Q2', 'prStd_Q2', 'cpStd_Q2', 'doStd_Q2', 'prMes_R1', 'doMes_R1', 'cpMes_R1', 'cn0_R1', 'prStd_R1', 'cpStd_R1', 'doStd_R1', 'prMes_R2', 'doMes_R2', 'cpMes_R2', 'cn0_R2', 'prStd_R2', 'cpStd_R2', 'doStd_R2']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'lat'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Removed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremoved\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremoved/original_len*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m gnss_clean = \u001b[43mclean_gnss_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnss_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(gnss_clean.describe())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mclean_gnss_data\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[clean_gnss_data] Columns:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(df.columns))\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Remove invalid lat/lon\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df = df[(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.between(-\u001b[32m90\u001b[39m, \u001b[32m90\u001b[39m)) & (df[\u001b[33m'\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m'\u001b[39m].between(-\u001b[32m180\u001b[39m, \u001b[32m180\u001b[39m))]\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Drop rows with missing critical fields\u001b[39;00m\n\u001b[32m     14\u001b[39m df = df.dropna(subset=[\u001b[33m'\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'lat'"
     ]
    }
   ],
   "source": [
    "# Data cleaning for GNSS\n",
    "\n",
    "def clean_gnss_data(df):\n",
    "    \"\"\"\n",
    "    Clean GNSS data by removing invalid coordinates, handling missing values, and filtering outliers.\n",
    "    \"\"\"\n",
    "    original_len = len(df)\n",
    "    df = df.copy()\n",
    "    # Print columns for debugging\n",
    "    print(\"[clean_gnss_data] Columns:\", list(df.columns))\n",
    "    # Remove invalid lat/lon\n",
    "    df = df[(df['lat'].between(-90, 90)) & (df['lon'].between(-180, 180))]\n",
    "    # Drop rows with missing critical fields\n",
    "    df = df.dropna(subset=['lat', 'lon', 'timestamp'])\n",
    "    # Remove outliers in speed (e.g., > 100 m/s)\n",
    "    if 'speed' in df.columns:\n",
    "        df = df[df['speed'] <= 100]\n",
    "    removed = original_len - len(df)\n",
    "    print(f\"Data Cleaning:\")\n",
    "    print(f\"  Original records: {original_len:,}\")\n",
    "    print(f\"  After cleaning: {len(df):,}\")\n",
    "    print(f\"  Removed: {removed:,} ({removed/original_len*100:.2f}%)\")\n",
    "    return df\n",
    "\n",
    "gnss_clean = clean_gnss_data(gnss_df)\n",
    "print(gnss_clean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd069338",
   "metadata": {},
   "source": [
    "Output: [clean_gnss_data] Columns: ['recordTime', 'VSG', 'VSE', 'VSB', 'VSQ', 'VSR', 'prMes_G1', 'doMes_G1', 'cpMes_G1', 'cn0_G1', 'prStd_G1', 'cpStd_G1', 'doStd_G1', 'prMes_G2', 'doMes_G2', 'cpMes_G2', 'cn0_G2', 'prStd_G2', 'cpStd_G2', 'doStd_G2', 'prMes_E1', 'doMes_E1', 'cpMes_E1', 'cn0_E1', 'prStd_E1', 'cpStd_E1', 'doStd_E1', 'prMes_E2', 'doMes_E2', 'cpMes_E2', 'cn0_E2', 'prStd_E2', 'cpStd_E2', 'doStd_E2', 'prMes_B1', 'doMes_B1', 'cpMes_B1', 'cn0_B1', 'prStd_B1', 'cpStd_B1', 'doStd_B1', 'prMes_B2', 'doMes_B2', 'cpMes_B2', 'cn0_B2', 'prStd_B2', 'cpStd_B2', 'doStd_B2', 'prMes_Q1', 'doMes_Q1', 'cpMes_Q1', 'cn0_Q1', 'prStd_Q1', 'cpStd_Q1', 'doStd_Q1', 'prMes_Q2', 'doMes_Q2', 'cpMes_Q2', 'cn0_Q2', 'prStd_Q2', 'cpStd_Q2', 'doStd_Q2', 'prMes_R1', 'doMes_R1', 'cpMes_R1', 'cn0_R1', 'prStd_R1', 'cpStd_R1', 'doStd_R1', 'prMes_R2', 'doMes_R2', 'cpMes_R2', 'cn0_R2', 'prStd_R2', 'cpStd_R2', 'doStd_R2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9287cc5",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering for GNSS Spoofing Detection\n",
    "\n",
    "Extract features such as latitude, longitude, speed, course, time-based features, and calculate movement deltas. Fill NaN values and display feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f078e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, pandas.core.series.Series)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gnss_df[\"recordTime\"]), type(gnss_df[\"recordTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1e2ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<class 'list'> is not convertible to datetime, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df, features\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# gnss_featured, feature_cols = extract_gnss_features(gnss_clean)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m gnss_featured, feature_cols = \u001b[43mextract_gnss_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnss_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(gnss_featured[feature_cols].head(\u001b[32m10\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mextract_gnss_features\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m      9\u001b[39m     features.append(\u001b[33m'\u001b[39m\u001b[33mcourse\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Time-based features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mrecordTime\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrecordTime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mhour\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mrecordTime\u001b[39m\u001b[33m'\u001b[39m].dt.hour\n\u001b[32m     13\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mday_of_week\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mrecordTime\u001b[39m\u001b[33m'\u001b[39m].dt.dayofweek\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\tools\\datetimes.py:1072\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1070\u001b[39m         result = arg.map(cache_array)\n\u001b[32m   1071\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m         values = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m         result = arg._constructor(values, index=arg.index, name=arg.name)\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc.MutableMapping)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\tools\\datetimes.py:437\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m result, tz_parsed = \u001b[43mobjects_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[32m    449\u001b[39m     out_unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\datetimes.py:2415\u001b[39m, in \u001b[36mobjects_to_datetime64\u001b[39m\u001b[34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[39m\n\u001b[32m   2412\u001b[39m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[32m   2413\u001b[39m data = np.asarray(data, dtype=np.object_)\n\u001b[32m-> \u001b[39m\u001b[32m2415\u001b[39m result, tz_parsed = \u001b[43mtslib\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2417\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreso\u001b[49m\u001b[43m=\u001b[49m\u001b[43mabbrev_to_npy_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2425\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m   2426\u001b[39m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[32m   2427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:412\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:596\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:588\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: <class 'list'> is not convertible to datetime, at position 0"
     ]
    }
   ],
   "source": [
    "# Feature extraction for GNSS spoofing detection\n",
    "\n",
    "def extract_gnss_features(df):\n",
    "    df = df.copy()\n",
    "    features = ['lat', 'lon']\n",
    "    if 'speed' in df.columns:\n",
    "        features.append('speed')\n",
    "    if 'course' in df.columns:\n",
    "        features.append('course')\n",
    "    # Time-based features\n",
    "    df['recordTime'] = pd.to_datetime(df['recordTime'])\n",
    "    df['hour'] = df['recordTime'].dt.hour\n",
    "    df['day_of_week'] = df['recordTime'].dt.dayofweek\n",
    "    features.extend(['hour', 'day_of_week'])\n",
    "    # Movement deltas\n",
    "    df = df.sort_values(['device_id', 'recordTime']) if 'device_id' in df.columns else df.sort_values('recordTime')\n",
    "    df['lat_diff'] = df['lat'].diff()\n",
    "    df['lon_diff'] = df['lon'].diff()\n",
    "    df['distance'] = np.sqrt(df['lat_diff']**2 + df['lon_diff']**2)\n",
    "    features.append('distance')\n",
    "    # Fill NaN values\n",
    "    df[features] = df[features].fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
    "    print(f\"Feature Extraction Complete:\")\n",
    "    print(f\"  Features: {features}\")\n",
    "    print(f\"  Feature matrix shape: {df[features].shape}\")\n",
    "    return df, features\n",
    "\n",
    "# gnss_featured, feature_cols = extract_gnss_features(gnss_clean)\n",
    "gnss_featured, feature_cols = extract_gnss_features(gnss_df)\n",
    "print(gnss_featured[feature_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b5fbf",
   "metadata": {},
   "source": [
    "## 5. Sequence Creation for LSTM\n",
    "\n",
    "Create temporal sequences from the feature matrix for LSTM input. Use sliding windows and assign spoofing labels to sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ceaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence creation for LSTM\n",
    "\n",
    "def create_gnss_sequences(df, feature_cols, sequence_length=128, stride=32, label_col='is_spoofed'):\n",
    "    X_list, y_list = [], []\n",
    "    # If device_id exists, group by device, else treat as single trajectory\n",
    "    group_key = 'device_id' if 'device_id' in df.columns else None\n",
    "    groups = df.groupby(group_key) if group_key else [(None, df)]\n",
    "    for _, group in groups:\n",
    "        group = group.sort_values('timestamp')\n",
    "        features = group[feature_cols].values\n",
    "        labels = group[label_col].values if label_col in group.columns else np.zeros(len(group))\n",
    "        if len(features) < sequence_length:\n",
    "            continue\n",
    "        for i in range(0, len(features) - sequence_length + 1, stride):\n",
    "            seq_x = features[i:i+sequence_length]\n",
    "            seq_y = labels[i:i+sequence_length]\n",
    "            # Label as spoofed if majority of points are spoofed\n",
    "            is_spoofed = int(seq_y.sum() > (sequence_length // 2))\n",
    "            X_list.append(seq_x)\n",
    "            y_list.append(is_spoofed)\n",
    "    X_array = np.array(X_list, dtype=np.float32)\n",
    "    y_array = np.array(y_list, dtype=np.uint8)\n",
    "    print(f\"Sequence Creation:\")\n",
    "    print(f\"  Sequence length: {sequence_length}\")\n",
    "    print(f\"  Stride: {stride}\")\n",
    "    print(f\"  Total sequences: {len(X_array):,}\")\n",
    "    print(f\"  Spoofed sequences: {(y_array == 1).sum():,}\")\n",
    "    print(f\"  Genuine sequences: {(y_array == 0).sum():,}\")\n",
    "    return X_array, y_array\n",
    "\n",
    "# Example: If you have spoofing labels, add a column 'is_spoofed' to gnss_featured, else all zeros\n",
    "gnss_featured['is_spoofed'] = 0  # Replace with actual labels if available\n",
    "SEQUENCE_LENGTH = 128\n",
    "STRIDE = 32\n",
    "X_sequences, y_labels = create_gnss_sequences(gnss_featured, feature_cols, sequence_length=SEQUENCE_LENGTH, stride=STRIDE, label_col='is_spoofed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90278b7e",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split\n",
    "\n",
    "Split the sequences into training, validation, and test sets. Ensure no data leakage by splitting by device or trajectory if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test/validation split\n",
    "# If possible, split by device_id to avoid leakage. Otherwise, use stratified random split.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.125, random_state=42, stratify=y_train  # 0.125 * 0.8 = 0.1 of total\n",
    ")\n",
    "print(f\"Data Split:\")\n",
    "print(f\"  Train: {len(X_train):,} sequences\")\n",
    "print(f\"  Val:   {len(X_val):,} sequences\")\n",
    "print(f\"  Test:  {len(X_test):,} sequences\")\n",
    "print(f\"  Train - Genuine: {(y_train==0).sum()}, Spoofed: {(y_train==1).sum()}\")\n",
    "print(f\"  Val   - Genuine: {(y_val==0).sum()}, Spoofed: {(y_val==1).sum()}\")\n",
    "print(f\"  Test  - Genuine: {(y_test==0).sum()}, Spoofed: {(y_test==1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee1bfe",
   "metadata": {},
   "source": [
    "## 7. Bi-LSTM Model Definition (PyTorch)\n",
    "\n",
    "Define the Bi-LSTM model architecture in PyTorch, including input, LSTM layers, and output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64783035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-LSTM model definition for GNSS spoofing detection\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, lstm_units_1=62, lstm_units_2=30):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.bilstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=lstm_units_1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.bilstm2 = nn.LSTM(\n",
    "            input_size=lstm_units_1 * 2,\n",
    "            hidden_size=lstm_units_2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.fc = nn.Linear(lstm_units_2 * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        lstm1_out, _ = self.bilstm1(x)\n",
    "        lstm2_out, _ = self.bilstm2(lstm1_out)\n",
    "        last_hidden = lstm2_out[:, -1, :]\n",
    "        output = self.fc(last_hidden)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "def build_bilstm_model(input_size, lstm_units_1=62, lstm_units_2=30):\n",
    "    model = BiLSTMModel(input_size, lstm_units_1, lstm_units_2)\n",
    "    return model.to(device)\n",
    "\n",
    "print(\"Building Bi-LSTM Model (PyTorch)...\")\n",
    "print(f\"  Input size: {X_train.shape[2]} features\")\n",
    "model = build_bilstm_model(input_size=X_train.shape[2])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401bef3",
   "metadata": {},
   "source": [
    "## 8. Model Training Loop\n",
    "\n",
    "Implement the training loop with weighted loss for class imbalance, early stopping, and metric calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7320934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 30\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.004\n",
    "PATIENCE = 10\n",
    "\n",
    "# Class weights for imbalance\n",
    "n_genuine = (y_train == 0).sum()\n",
    "n_spoofed = (y_train == 1).sum()\n",
    "total_samples = len(y_train)\n",
    "weight_genuine = total_samples / (2.0 * n_genuine)\n",
    "weight_spoofed = total_samples / (2.0 * n_spoofed) if n_spoofed > 0 else 1.0\n",
    "\n",
    "print(f\"Class Distribution in Training Set:\")\n",
    "print(f\"  Genuine: {n_genuine:,} ({n_genuine/total_samples*100:.2f}%)\")\n",
    "print(f\"  Spoofed: {n_spoofed:,} ({n_spoofed/total_samples*100:.2f}%)\")\n",
    "print(f\"Class Weights:\")\n",
    "print(f\"  Genuine weight: {weight_genuine:.4f}\")\n",
    "print(f\"  Spoofed weight: {weight_spoofed:.4f}\")\n",
    "\n",
    "# Data loaders\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float().reshape(-1, 1))\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float().reshape(-1, 1))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Weighted BCE loss\n",
    "pos_weight = torch.tensor([weight_spoofed / weight_genuine]).to(device) if n_spoofed > 0 else torch.tensor([1.0]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if n_spoofed > 0 else nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop with early stopping\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': [], 'train_precision': [], 'val_precision': [], 'train_recall': [], 'val_recall': []}\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Early stopping patience: {PATIENCE}\")\n",
    "print(f\"  Loss function: BCEWithLogitsLoss (weighted)\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"\\nStarting training...\\n\")\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    for batch_x, batch_y in loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "        all_preds.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "        all_labels.extend(batch_y.detach().cpu().numpy())\n",
    "    return total_loss / len(loader.dataset), np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item() * batch_x.size(0)\n",
    "            all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "    return total_loss / len(loader.dataset), np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_preds, train_labels = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "    train_preds_binary = (train_preds > 0.5).astype(int)\n",
    "    val_preds_binary = (val_preds > 0.5).astype(int)\n",
    "    train_acc = (train_preds_binary == train_labels).mean()\n",
    "    val_acc = (val_preds_binary == val_labels).mean()\n",
    "    train_prec = precision_score(train_labels, train_preds_binary, zero_division=0)\n",
    "    val_prec = precision_score(val_labels, val_preds_binary, zero_division=0)\n",
    "    train_rec = recall_score(train_labels, train_preds_binary, zero_division=0)\n",
    "    val_rec = recall_score(val_labels, val_preds_binary, zero_division=0)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_accuracy'].append(train_acc)\n",
    "    history['val_accuracy'].append(val_acc)\n",
    "    history['train_precision'].append(train_prec)\n",
    "    history['val_precision'].append(val_prec)\n",
    "    history['train_recall'].append(train_rec)\n",
    "    history['val_recall'].append(val_rec)\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Recall: {val_rec:.4f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1f1d7",
   "metadata": {},
   "source": [
    "## 9. Training History Visualization\n",
    "\n",
    "Plot training and validation loss, accuracy, precision, and recall over epochs using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0893f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Model Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "# Precision & Recall\n",
    "axes[2].plot(history['train_precision'], label='Train Precision', linewidth=2)\n",
    "axes[2].plot(history['val_precision'], label='Val Precision', linewidth=2, linestyle='--')\n",
    "axes[2].plot(history['train_recall'], label='Train Recall', linewidth=2)\n",
    "axes[2].plot(history['val_recall'], label='Val Recall', linewidth=2, linestyle='--')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Score')\n",
    "axes[2].set_title('Precision & Recall')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba05e90",
   "metadata": {},
   "source": [
    "## 10. Threshold Optimization\n",
    "\n",
    "Find the optimal decision threshold on the validation set by maximizing F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold optimization on validation set\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "best_metrics = {}\n",
    "val_probs_all = []\n",
    "val_labels_all = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in val_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x).cpu().numpy().flatten()\n",
    "        val_probs_all.extend(outputs)\n",
    "        val_labels_all.extend(batch_y.numpy().flatten())\n",
    "val_probs_all = np.array(val_probs_all)\n",
    "val_labels_all = np.array(val_labels_all)\n",
    "print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for threshold in thresholds:\n",
    "    preds = (val_probs_all > threshold).astype(int)\n",
    "    f1 = f1_score(val_labels_all, preds, zero_division=0)\n",
    "    precision = precision_score(val_labels_all, preds, zero_division=0)\n",
    "    recall = recall_score(val_labels_all, preds, zero_division=0)\n",
    "    print(f\"τ={threshold:<8.2f}   {precision:<12.4f} {recall:<12.4f} {f1:<12.4f}\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "        best_metrics = {'f1': f1, 'precision': precision, 'recall': recall}\n",
    "THRESHOLD = best_threshold\n",
    "print(f\"\\nOptimal Threshold: {THRESHOLD:.2f}\")\n",
    "if best_metrics:\n",
    "    print(f\"  F1-Score:  {best_metrics['f1']:.4f}\")\n",
    "    print(f\"  Precision: {best_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {best_metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24244cb2",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation on Test Set\n",
    "\n",
    "Evaluate the trained model on the test set. Calculate accuracy, precision, recall, F1-score, and detection rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cea9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "BATCH_SIZE_INFERENCE = 256\n",
    "model.eval()\n",
    "y_pred_probs = []\n",
    "test_losses = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_test), BATCH_SIZE_INFERENCE):\n",
    "        batch_X = torch.from_numpy(X_test[i:i+BATCH_SIZE_INFERENCE]).float().to(device)\n",
    "        batch_y = torch.from_numpy(y_test[i:i+BATCH_SIZE_INFERENCE]).float().reshape(-1, 1).to(device)\n",
    "        batch_probs = model(batch_X)\n",
    "        y_pred_probs.extend(batch_probs.cpu().numpy())\n",
    "        batch_loss = criterion(batch_probs, batch_y)\n",
    "        test_losses.append(batch_loss.item() * len(batch_X))\n",
    "y_pred_probs = np.array(y_pred_probs).flatten()\n",
    "y_pred = (y_pred_probs > THRESHOLD).astype(int)\n",
    "test_loss = sum(test_losses) / len(X_test)\n",
    "test_acc = (y_pred == y_test).mean()\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "detection_rate = recall\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(f\"  Accuracy:       {test_acc:.4f}\")\n",
    "print(f\"  Precision:      {precision:.4f}\")\n",
    "print(f\"  Recall:         {recall:.4f}\")\n",
    "print(f\"  F1-Score:       {f1:.4f}\")\n",
    "print(f\"  Detection Rate: {detection_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd36ad8",
   "metadata": {},
   "source": [
    "## 12. Confusion Matrix and Classification Report\n",
    "\n",
    "Plot the confusion matrix and print the detailed classification report for test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c03430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Genuine', 'Spoofed'])\n",
    "ax.set_yticklabels(['Genuine', 'Spoofed'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\", fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Genuine', 'Spoofed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7773a9",
   "metadata": {},
   "source": [
    "## 13. Model Export (Save Model and Scaler)\n",
    "\n",
    "Save the trained PyTorch model, feature scaler, and configuration to disk for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d578176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model artifacts\n",
    "import pickle\n",
    "output_root = Path(\"./models\")\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "model_path = output_root / 'gnss_bilstm_model.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "# Save scaler (if used)\n",
    "# scaler_path = output_root / 'feature_scaler.pkl'\n",
    "# with open(scaler_path, 'wb') as f:\n",
    "#     pickle.dump(feature_scaler, f)\n",
    "# Save configuration\n",
    "config = {\n",
    "    'feature_cols': feature_cols,\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'threshold': THRESHOLD,\n",
    "    'lstm_units_1': 62,\n",
    "    'lstm_units_2': 30,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'input_size': X_train.shape[2],\n",
    "    'test_metrics': {\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'detection_rate': float(detection_rate),\n",
    "        'accuracy': float(test_acc)\n",
    "    }\n",
    "}\n",
    "config_path = output_root / 'gnss_model_config.pkl'\n",
    "with open(config_path, 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Config saved to: {config_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
