{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5097b07a",
   "metadata": {},
   "source": [
    "# Bi-LSTM Spoofing Detection for Maritime AIS Data\n",
    "\n",
    "Implementation based on \"Vessel Trajectory Route Spoofed Points Detection Using AIS Data: A Bi-LSTM Approach\" (Raj & Kumar, 2025)\n",
    "\n",
    "**Methodology:**\n",
    "1. AIS Data Loading from Full Dataset (2017-2019)\n",
    "2. Data Preprocessing & Feature Extraction\n",
    "3. Spoofed Point Generation (Data Augmentation)\n",
    "4. Bi-LSTM Model Architecture\n",
    "5. Training & Evaluation\n",
    "6. Anomaly Detection & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17cb007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu126\n",
      "GPU Available: True\n",
      "GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Numpy version: 1.26.0\n",
      "Pandas version: 2.3.3\n",
      "All packages loaded successfully\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and imports\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  # Fix OpenMP conflict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Visualization\n",
    "import folium\n",
    "from folium import plugins\n",
    "import itertools\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(\"All packages loaded successfully\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb91013",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Preprocessing\n",
    "\n",
    "Load AIS data from incident slices created in the incident analysis notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1c5abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "Data root: ..\\dataset\\piraeus\n",
      "Loading periods: 2017-2019 (multiple months)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "data_root = Path(\"../dataset/piraeus\")\n",
    "output_root = Path(\"./models\")\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# AIS dataset configuration (from incident_anomaly_labels.ipynb)\n",
    "cols_primary = [\"timestamp\", \"vessel_id\", \"lon\", \"lat\", \"speed\", \"course\", \"heading\"]\n",
    "cols_alias = [\"t\", \"timestamp\", \"vessel_id\", \"lon\", \"lat\", \"speed\", \"course\", \"heading\"]\n",
    "\n",
    "MONTH_ABBR = {\n",
    "    1: \"jan\", 2: \"feb\", 3: \"mar\", 4: \"apr\", 5: \"may\", 6: \"jun\",\n",
    "    7: \"jul\", 8: \"aug\", 9: \"sep\", 10: \"oct\", 11: \"nov\", 12: \"dec\"\n",
    "}\n",
    "\n",
    "# Years and months to load for comprehensive training\n",
    "DATA_PERIODS = [\n",
    "    (2017, [5, 6, 7, 8, 9, 10, 11, 12]),  # May-Dec 2017\n",
    "    (2018, list(range(1, 13))),            # Full year 2018\n",
    "    (2019, list(range(1, 13))),            # Full year 2019\n",
    "]\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"Data root: {data_root}\")\n",
    "print(f\"Loading periods: 2017-2019 (multiple months)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a833f674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AIS dataset...\n",
      "Loading 2017-10...\n",
      "  Loaded: 4,286,717 records, 816 vessels\n",
      "Loading 2017-05...\n",
      "  Loaded: 4,305,035 records, 778 vessels\n",
      "Loading 2017-07...\n",
      "  Loaded: 8,340,504 records, 1135 vessels\n",
      "Loading 2018-04...\n",
      "  Loaded: 7,608,586 records, 884 vessels\n",
      "Loading 2018-01...\n",
      "  Loaded: 5,458,182 records, 659 vessels\n",
      "Loading 2018-12...\n",
      "  Loaded: 4,690,581 records, 848 vessels\n",
      "Loading 2019-10...\n",
      "  Loaded: 10,944,763 records, 1262 vessels\n",
      "Loading 2019-02...\n",
      "  Loaded: 6,930,667 records, 874 vessels\n",
      "Loading 2019-01...\n",
      "  Loaded: 4,480,731 records, 782 vessels\n",
      "\n",
      "Total months loaded: 9\n",
      "\n",
      "Dataset Summary:\n",
      "  Total records: 57,045,766\n",
      "  Unique vessels: 3846\n",
      "  Time range: 2017-05-09 14:05:26+00:00 to 2019-10-30 16:01:07+00:00\n",
      "  Columns: ['timestamp', 'vessel_id', 'lon', 'lat', 'speed', 'course', 'heading']\n"
     ]
    }
   ],
   "source": [
    "# Load AIS data from full dataset\n",
    "def load_month(year, month, root, chunk_size=500_000):\n",
    "    \"\"\"Load AIS data for a specific month (from incident_anomaly_labels.ipynb).\"\"\"\n",
    "    folder = Path(root) / f\"unipi_ais_dynamic_{year}\"\n",
    "    fname = folder / f\"unipi_ais_dynamic_{MONTH_ABBR[month]}{year}.csv\"\n",
    "    if not fname.exists():\n",
    "        alt = folder / f\"unipi_ais_dynamic_{year}_{month:02d}.csv\"\n",
    "        if alt.exists():\n",
    "            fname = alt\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Missing file: {fname} (or {alt})\")\n",
    "\n",
    "    # Discover available columns\n",
    "    preview = pd.read_csv(fname, nrows=1)\n",
    "    available = list(preview.columns)\n",
    "    selected_cols = [c for c in cols_alias if c in available]\n",
    "\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(fname, usecols=selected_cols, chunksize=chunk_size):\n",
    "        if \"t\" in chunk.columns and \"timestamp\" not in chunk.columns:\n",
    "            chunk = chunk.rename(columns={\"t\": \"timestamp\"})\n",
    "        chunk[\"timestamp\"] = pd.to_datetime(chunk[\"timestamp\"], unit=\"ms\", utc=True)\n",
    "        chunk = chunk[[c for c in cols_primary if c in chunk.columns]]\n",
    "        chunks.append(chunk)\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "def load_ais_dataset(data_periods, root, sample_months=3):\n",
    "    \"\"\"Load multiple months of AIS data for training.\"\"\"\n",
    "    all_data = []\n",
    "    loaded_count = 0\n",
    "\n",
    "    for year, months in data_periods:\n",
    "        # Sample subset of months to manage memory\n",
    "        selected_months = np.random.choice(months, min(sample_months, len(months)), replace=False)\n",
    "        \n",
    "        for month in selected_months:\n",
    "            try:\n",
    "                print(f\"Loading {year}-{month:02d}...\")\n",
    "                df = load_month(year, month, root)\n",
    "                all_data.append(df)\n",
    "                loaded_count += 1\n",
    "                print(f\"  Loaded: {len(df):,} records, {df['vessel_id'].nunique()} vessels\")\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"  Skipped: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not all_data:\n",
    "        raise FileNotFoundError(\"No AIS data found. Check data_root path.\")\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"\\nTotal months loaded: {loaded_count}\")\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "# Load data (sample 3 months per year for manageable dataset size)\n",
    "print(\"Loading AIS dataset...\")\n",
    "ais_data = load_ais_dataset(DATA_PERIODS, data_root, sample_months=3)\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  Total records: {len(ais_data):,}\")\n",
    "print(f\"  Unique vessels: {ais_data['vessel_id'].nunique()}\")\n",
    "print(f\"  Time range: {ais_data['timestamp'].min()} to {ais_data['timestamp'].max()}\")\n",
    "print(f\"  Columns: {list(ais_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9088a036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning:\n",
      "  Original records: 57,045,766\n",
      "  After cleaning: 30,358,099\n",
      "  Removed: 26,687,667 (46.78%)\n",
      "\n",
      "Data Integrity Check:\n",
      "  Lat range: [37.4595, 38.0423]\n",
      "  Lon range: [23.0240, 24.0169]\n",
      "  Speed range: [0.00, 99.90] knots\n",
      "  Course range: [0.00, 359.90] degrees\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning (Algorithm 1 from paper: domain-specific constraints)\n",
    "def clean_ais_data(df):\n",
    "    \"\"\"Apply domain-specific constraints as per paper.\"\"\"\n",
    "    original_len = len(df)\n",
    "    \n",
    "    # Rule-based cleaning from paper\n",
    "    df = df.copy()\n",
    "    \n",
    "    # COG should not exceed 360 degrees\n",
    "    if 'course' in df.columns:\n",
    "        df = df[df['course'] <= 360]\n",
    "    \n",
    "    # SOG should not exceed 100 knots\n",
    "    if 'speed' in df.columns:\n",
    "        df = df[df['speed'] <= 100]\n",
    "    \n",
    "    # Remove null values in critical fields\n",
    "    df = df.dropna(subset=['lat', 'lon', 'timestamp', 'vessel_id'])\n",
    "    \n",
    "    # Remove duplicate timestamps per vessel\n",
    "    df = df.sort_values(['vessel_id', 'timestamp'])\n",
    "    df = df.drop_duplicates(subset=['vessel_id', 'timestamp'], keep='first')\n",
    "    \n",
    "    # Remove stationary vessels (speed = 0 for extended periods)\n",
    "    # Keep vessels with at least some movement\n",
    "    vessel_max_speed = df.groupby('vessel_id')['speed'].max()\n",
    "    moving_vessels = vessel_max_speed[vessel_max_speed > 0.5].index\n",
    "    df = df[df['vessel_id'].isin(moving_vessels)]\n",
    "    \n",
    "    removed = original_len - len(df)\n",
    "    print(f\"Data Cleaning:\")\n",
    "    print(f\"  Original records: {original_len:,}\")\n",
    "    print(f\"  After cleaning: {len(df):,}\")\n",
    "    print(f\"  Removed: {removed:,} ({removed/original_len*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean data\n",
    "ais_clean = clean_ais_data(ais_data)\n",
    "\n",
    "# Verify data integrity\n",
    "print(f\"\\nData Integrity Check:\")\n",
    "print(f\"  Lat range: [{ais_clean['lat'].min():.4f}, {ais_clean['lat'].max():.4f}]\")\n",
    "print(f\"  Lon range: [{ais_clean['lon'].min():.4f}, {ais_clean['lon'].max():.4f}]\")\n",
    "print(f\"  Speed range: [{ais_clean['speed'].min():.2f}, {ais_clean['speed'].max():.2f}] knots\")\n",
    "if 'course' in ais_clean.columns:\n",
    "    print(f\"  Course range: [{ais_clean['course'].min():.2f}, {ais_clean['course'].max():.2f}] degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "677d9951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned data saved to: ..\\dataset\\piraeus\\ais_cleaned.parquet\n"
     ]
    }
   ],
   "source": [
    "# save in parquet format for faster loading next time\n",
    "parquet_path = data_root / \"ais_cleaned.parquet\"\n",
    "if !parquet_path.exists():\n",
    "    # parquet_path.unlink()  # Remove existing file\n",
    "    ais_clean.to_parquet(parquet_path, index=False)\n",
    "    print(f\"\\nCleaned data saved to: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b19ec5",
   "metadata": {},
   "source": [
    "## 2. Spoofed Point Generation (Data Augmentation)\n",
    "\n",
    "Generate synthetic spoofed trajectories using the paper's three classical cases:\n",
    "1. **Off-course deviation**: Random bearing shifts (±30-90°)\n",
    "2. **Track deviation**: Gradual position drift with Gaussian noise\n",
    "3. **CPA violations**: Closest-point-of-approach anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "029e8469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spoofing generation functions defined\n",
      "  - Off-course deviation (bearing shift)\n",
      "  - Track deviation (gradual drift)\n",
      "  - CPA violation (speed/course anomalies)\n"
     ]
    }
   ],
   "source": [
    "# Spoofing generation utilities\n",
    "def generate_off_course_spoofing(df, spoof_ratio=0.15):\n",
    "    \"\"\"Generate off-course deviation spoofing (random bearing shift).\"\"\"\n",
    "    df_spoofed = df.copy()\n",
    "    n_spoof = int(len(df) * spoof_ratio)\n",
    "    spoof_indices = np.random.choice(df.index, size=n_spoof, replace=False)\n",
    "    \n",
    "    # Random bearing shift (30-90 degrees)\n",
    "    bearing_shift = np.random.uniform(30, 90, size=n_spoof)\n",
    "    direction = np.random.choice([-1, 1], size=n_spoof)\n",
    "    bearing_shift = bearing_shift * direction\n",
    "    \n",
    "    # Apply shift to coordinates (approximate, in degrees)\n",
    "    # 1 degree ≈ 111 km at equator\n",
    "    distance_km = np.random.uniform(0.5, 3.0, size=n_spoof)  # 0.5-3 km shift\n",
    "    \n",
    "    df_spoofed.loc[spoof_indices, 'lat'] += (distance_km / 111.0) * np.cos(np.radians(bearing_shift))\n",
    "    df_spoofed.loc[spoof_indices, 'lon'] += (distance_km / (111.0 * np.cos(np.radians(df.loc[spoof_indices, 'lat'])))) * np.sin(np.radians(bearing_shift))\n",
    "    \n",
    "    return df_spoofed, spoof_indices\n",
    "\n",
    "\n",
    "def generate_track_deviation_spoofing(df, spoof_ratio=0.15):\n",
    "    \"\"\"Generate track deviation spoofing (gradual drift with Gaussian noise).\"\"\"\n",
    "    df_spoofed = df.copy()\n",
    "    \n",
    "    # Select random vessels for spoofing\n",
    "    vessels = df['vessel_id'].unique()\n",
    "    n_vessels_spoof = max(1, int(len(vessels) * spoof_ratio))\n",
    "    spoof_vessels = np.random.choice(vessels, size=n_vessels_spoof, replace=False)\n",
    "    \n",
    "    spoof_indices = []\n",
    "    \n",
    "    for vessel in spoof_vessels:\n",
    "        vessel_mask = df['vessel_id'] == vessel\n",
    "        vessel_data = df[vessel_mask].sort_values('timestamp')\n",
    "        \n",
    "        if len(vessel_data) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Apply gradual drift to middle portion of trajectory\n",
    "        start_idx = len(vessel_data) // 4\n",
    "        end_idx = 3 * len(vessel_data) // 4\n",
    "        drift_indices = vessel_data.iloc[start_idx:end_idx].index\n",
    "        \n",
    "        # Gaussian noise with increasing magnitude\n",
    "        drift_length = len(drift_indices)\n",
    "        lat_drift = np.cumsum(np.random.normal(0, 0.0005, drift_length))\n",
    "        lon_drift = np.cumsum(np.random.normal(0, 0.0005, drift_length))\n",
    "        \n",
    "        df_spoofed.loc[drift_indices, 'lat'] += lat_drift\n",
    "        df_spoofed.loc[drift_indices, 'lon'] += lon_drift\n",
    "        \n",
    "        spoof_indices.extend(drift_indices.tolist())\n",
    "    \n",
    "    return df_spoofed, spoof_indices\n",
    "\n",
    "\n",
    "def generate_cpa_violation_spoofing(df, spoof_ratio=0.15):\n",
    "    \"\"\"Generate CPA (Closest Point of Approach) violation spoofing.\"\"\"\n",
    "    df_spoofed = df.copy()\n",
    "    n_spoof = int(len(df) * spoof_ratio)\n",
    "    spoof_indices = np.random.choice(df.index, size=n_spoof, replace=False)\n",
    "    \n",
    "    # Simulate collision-course trajectories (sudden speed/course changes)\n",
    "    df_spoofed.loc[spoof_indices, 'speed'] *= np.random.uniform(1.5, 3.0, size=n_spoof)\n",
    "    \n",
    "    if 'course' in df.columns:\n",
    "        # Sudden course change\n",
    "        df_spoofed.loc[spoof_indices, 'course'] += np.random.uniform(-45, 45, size=n_spoof)\n",
    "        df_spoofed.loc[spoof_indices, 'course'] = df_spoofed.loc[spoof_indices, 'course'] % 360\n",
    "    \n",
    "    return df_spoofed, spoof_indices\n",
    "\n",
    "\n",
    "print(\"Spoofing generation functions defined\")\n",
    "print(\"  - Off-course deviation (bearing shift)\")\n",
    "print(\"  - Track deviation (gradual drift)\")\n",
    "print(\"  - CPA violation (speed/course anomalies)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb64472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Dataset Statistics:\n",
      "  Total records: 30,358,099\n",
      "  Genuine: 23,413,778 (77.1%)\n",
      "  Spoofed: 6,944,321 (22.9%)\n",
      "  - Off-course indices: 3,035,809\n",
      "  - Track deviation indices: 1,451,125\n",
      "  - CPA violation indices: 3,035,809\n"
     ]
    }
   ],
   "source": [
    "# Generate augmented dataset with spoofed points (memory-optimized)\n",
    "def create_augmented_dataset(df, spoof_ratio=0.3, chunk_size=1_000_000):\n",
    "    \"\"\"\n",
    "    Create augmented dataset with genuine and spoofed AIS messages, optimized for large DataFrames.\n",
    "    - Uses NumPy arrays and chunked assignment to avoid pandas reindex memory blow-ups.\n",
    "    - Downcasts numeric columns to float32 to reduce memory usage.\n",
    "    - Resets the index so NumPy positional arrays align with spoof indices.\n",
    "\n",
    "    Args:\n",
    "        df: Clean AIS data\n",
    "        spoof_ratio: Proportion of data to spoof (0.3 = 30%)\n",
    "        chunk_size: Number of indices per chunk during assignment\n",
    "\n",
    "    Returns:\n",
    "        Augmented dataframe with 'is_spoofed' label\n",
    "    \"\"\"\n",
    "    # Ensure 0-based integer indices for safe NumPy indexing\n",
    "    df_augmented = df.reset_index(drop=True).copy()\n",
    "    df_augmented['is_spoofed'] = 0\n",
    "\n",
    "    # Split spoofing equally among three methods\n",
    "    ratio_per_method = float(spoof_ratio) / 3.0\n",
    "\n",
    "    # Generate spoofed data using each method\n",
    "    df_off_course, indices_off = generate_off_course_spoofing(df_augmented, ratio_per_method)\n",
    "    df_track_dev, indices_track = generate_track_deviation_spoofing(df_augmented, ratio_per_method)\n",
    "    df_cpa, indices_cpa = generate_cpa_violation_spoofing(df_augmented, ratio_per_method)\n",
    "\n",
    "    # Convert to NumPy arrays (downcast to float32 to reduce memory)\n",
    "    lat = df_augmented['lat'].to_numpy(dtype=np.float32, copy=True)\n",
    "    lon = df_augmented['lon'].to_numpy(dtype=np.float32, copy=True)\n",
    "    speed = df_augmented['speed'].to_numpy(dtype=np.float32, copy=True)\n",
    "    course = df_augmented['course'].to_numpy(dtype=np.float32, copy=True) if 'course' in df_augmented.columns else None\n",
    "\n",
    "    lat_off = df_off_course['lat'].to_numpy(dtype=np.float32, copy=False)\n",
    "    lon_off = df_off_course['lon'].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    lat_track = df_track_dev['lat'].to_numpy(dtype=np.float32, copy=False)\n",
    "    lon_track = df_track_dev['lon'].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "    speed_cpa = df_cpa['speed'].to_numpy(dtype=np.float32, copy=False)\n",
    "    course_cpa = df_cpa['course'].to_numpy(dtype=np.float32, copy=False) if course is not None and 'course' in df_cpa.columns else None\n",
    "\n",
    "    # Helper to apply changes in chunks to avoid large temporary arrays\n",
    "    def apply_in_chunks(idx_array, setter_fn):\n",
    "        idx_array = np.asarray(idx_array, dtype=np.int64)\n",
    "        if idx_array.size == 0:\n",
    "            return\n",
    "        # Determine chunk count, at least 1\n",
    "        num_chunks = max(1, int(np.ceil(idx_array.size / chunk_size)))\n",
    "        for chunk in np.array_split(idx_array, num_chunks):\n",
    "            if chunk.size == 0:\n",
    "                continue\n",
    "            setter_fn(chunk)\n",
    "\n",
    "    # Apply off-course lat/lon updates\n",
    "    def set_off_course(chunk):\n",
    "        lat[chunk] = lat_off[chunk]\n",
    "        lon[chunk] = lon_off[chunk]\n",
    "    apply_in_chunks(indices_off, set_off_course)\n",
    "\n",
    "    # Apply track deviation lat/lon updates\n",
    "    def set_track_dev(chunk):\n",
    "        lat[chunk] = lat_track[chunk]\n",
    "        lon[chunk] = lon_track[chunk]\n",
    "    apply_in_chunks(indices_track, set_track_dev)\n",
    "\n",
    "    # Apply CPA speed/course updates\n",
    "    def set_cpa(chunk):\n",
    "        speed[chunk] = speed_cpa[chunk]\n",
    "        if course is not None and course_cpa is not None:\n",
    "            course[chunk] = course_cpa[chunk]\n",
    "    apply_in_chunks(indices_cpa, set_cpa)\n",
    "\n",
    "    # Write back arrays into DataFrame\n",
    "    df_augmented['lat'] = lat\n",
    "    df_augmented['lon'] = lon\n",
    "    df_augmented['speed'] = speed\n",
    "    if course is not None and 'course' in df_augmented.columns:\n",
    "        df_augmented['course'] = course\n",
    "\n",
    "    # Label spoofed points efficiently using a single mask\n",
    "    mask = np.zeros(len(df_augmented), dtype=np.uint8)\n",
    "    for idxs in (indices_off, indices_track, indices_cpa):\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "        apply_in_chunks(idxs, lambda ch: mask.__setitem__(ch, 1))\n",
    "    df_augmented['is_spoofed'] = mask\n",
    "\n",
    "    # Stats\n",
    "    total = len(df_augmented)\n",
    "    genuine = int((df_augmented['is_spoofed'] == 0).sum())\n",
    "    spoofed = total - genuine\n",
    "\n",
    "    print(\"Augmented Dataset Statistics:\")\n",
    "    print(f\"  Total records: {total:,}\")\n",
    "    print(f\"  Genuine: {genuine:,} ({genuine/total*100:.1f}%)\")\n",
    "    print(f\"  Spoofed: {spoofed:,} ({spoofed/total*100:.1f}%)\")\n",
    "    print(f\"  - Off-course indices: {len(indices_off):,}\")\n",
    "    print(f\"  - Track deviation indices: {len(indices_track):,}\")\n",
    "    print(f\"  - CPA violation indices: {len(indices_cpa):,}\")\n",
    "\n",
    "    return df_augmented\n",
    "\n",
    "# Generate augmented dataset\n",
    "np.random.seed(42)  # For reproducibility\n",
    "# If memory issues persist, reduce spoof_ratio (e.g., 0.1) or increase chunk_size\n",
    "ais_augmented = create_augmented_dataset(ais_clean, spoof_ratio=0.3, chunk_size=1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb1e38",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Sequence Creation\n",
    "\n",
    "Extract features (lat, lon, SOG, COG) and create temporal sequences for Bi-LSTM input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction (Algorithm 1: Feature Extraction step)\n",
    "def extract_features(df):\n",
    "    \"\"\"Extract relevant features for Bi-LSTM model.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Core features from paper\n",
    "    features = ['lat', 'lon', 'speed']\n",
    "    \n",
    "    if 'course' in df.columns:\n",
    "        features.append('course')\n",
    "    \n",
    "    # Temporal features\n",
    "    df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['timestamp']).dt.dayofweek\n",
    "    features.extend(['hour', 'day_of_week'])\n",
    "    \n",
    "    # Velocity components (if course available)\n",
    "    if 'course' in df.columns:\n",
    "        df['velocity_x'] = df['speed'] * np.cos(np.radians(df['course']))\n",
    "        df['velocity_y'] = df['speed'] * np.sin(np.radians(df['course']))\n",
    "        features.extend(['velocity_x', 'velocity_y'])\n",
    "    \n",
    "    # Calculate inter-point distances (movement magnitude)\n",
    "    df = df.sort_values(['vessel_id', 'timestamp'])\n",
    "    df['lat_diff'] = df.groupby('vessel_id')['lat'].diff()\n",
    "    df['lon_diff'] = df.groupby('vessel_id')['lon'].diff()\n",
    "    df['distance'] = np.sqrt(df['lat_diff']**2 + df['lon_diff']**2)\n",
    "    features.append('distance')\n",
    "    \n",
    "    # Fill NaN values from diff operations\n",
    "    df[features] = df[features].fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    print(f\"Feature Extraction Complete:\")\n",
    "    print(f\"  Features: {features}\")\n",
    "    print(f\"  Feature matrix shape: {df[features].shape}\")\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Extract features\n",
    "ais_featured, feature_cols = extract_features(ais_augmented)\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nSample Features:\")\n",
    "print(ais_featured[feature_cols + ['is_spoofed']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db77eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature normalization (Algorithm 1: Feature Scaling)\n",
    "def normalize_features(df, feature_cols, fit_scaler=True, scaler=None):\n",
    "    \"\"\"Normalize features using StandardScaler.\"\"\"\n",
    "    if fit_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "        print(f\"Features normalized (fitted new scaler)\")\n",
    "    else:\n",
    "        df[feature_cols] = scaler.transform(df[feature_cols])\n",
    "        print(f\"Features normalized (using existing scaler)\")\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "# Normalize features\n",
    "ais_normalized, feature_scaler = normalize_features(\n",
    "    ais_featured, \n",
    "    feature_cols, \n",
    "    fit_scaler=True\n",
    ")\n",
    "\n",
    "print(f\"\\nNormalized Feature Statistics:\")\n",
    "print(ais_normalized[feature_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for LSTM (temporal windows)\n",
    "def create_sequences(df, feature_cols, sequence_length=128, stride=32):\n",
    "    \"\"\"\n",
    "    Create temporal sequences for Bi-LSTM input.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe with features\n",
    "        feature_cols: List of feature column names\n",
    "        sequence_length: Number of timesteps per sequence\n",
    "        stride: Step size for sliding window\n",
    "    \n",
    "    Returns:\n",
    "        X: Feature sequences (n_samples, sequence_length, n_features)\n",
    "        y: Labels (n_samples,)\n",
    "    \"\"\"\n",
    "    X_sequences = []\n",
    "    y_labels = []\n",
    "    \n",
    "    # Group by vessel\n",
    "    for vessel_id, vessel_data in df.groupby('vessel_id'):\n",
    "        vessel_data = vessel_data.sort_values('timestamp')\n",
    "        \n",
    "        # Extract features and labels\n",
    "        features = vessel_data[feature_cols].values\n",
    "        labels = vessel_data['is_spoofed'].values\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if len(features) < sequence_length:\n",
    "            continue\n",
    "        \n",
    "        # Create sliding windows\n",
    "        for i in range(0, len(features) - sequence_length + 1, stride):\n",
    "            seq_features = features[i:i+sequence_length]\n",
    "            seq_label = labels[i:i+sequence_length]\n",
    "            \n",
    "            # Label sequence as spoofed if ANY point is spoofed\n",
    "            # (conservative approach for safety)\n",
    "            is_spoofed = int(seq_label.sum() > 0)\n",
    "            \n",
    "            X_sequences.append(seq_features)\n",
    "            y_labels.append(is_spoofed)\n",
    "    \n",
    "    X = np.array(X_sequences)\n",
    "    y = np.array(y_labels)\n",
    "    \n",
    "    print(f\"Sequence Creation:\")\n",
    "    print(f\"  Sequence length: {sequence_length} timesteps\")\n",
    "    print(f\"  Stride: {stride}\")\n",
    "    print(f\"  Total sequences: {len(X):,}\")\n",
    "    print(f\"  Genuine sequences: {(y == 0).sum():,} ({(y == 0).sum()/len(y)*100:.1f}%)\")\n",
    "    print(f\"  Spoofed sequences: {(y == 1).sum():,} ({(y == 1).sum()/len(y)*100:.1f}%)\")\n",
    "    print(f\"  Shape: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Create sequences\n",
    "SEQUENCE_LENGTH = 128\n",
    "STRIDE = 32\n",
    "\n",
    "X_sequences, y_labels = create_sequences(\n",
    "    ais_normalized, \n",
    "    feature_cols, \n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    stride=STRIDE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98832f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (Algorithm 1: train_test_split)\n",
    "# Split by vessel to avoid leakage\n",
    "def vessel_aware_split(X, y, df, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Split data by vessel to prevent data leakage.\n",
    "    \"\"\"\n",
    "    # Get unique vessels\n",
    "    vessels = df['vessel_id'].unique()\n",
    "    n_vessels = len(vessels)\n",
    "    \n",
    "    # Shuffle vessels\n",
    "    np.random.shuffle(vessels)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    test_idx = int(n_vessels * (1 - test_size))\n",
    "    val_idx = int(test_idx * (1 - val_size))\n",
    "    \n",
    "    train_vessels = vessels[:val_idx]\n",
    "    val_vessels = vessels[val_idx:test_idx]\n",
    "    test_vessels = vessels[test_idx:]\n",
    "    \n",
    "    # Create vessel-to-sequences mapping\n",
    "    vessel_to_seq = {}\n",
    "    for idx, (vessel_id, _) in enumerate(df.groupby('vessel_id')):\n",
    "        vessel_to_seq[vessel_id] = []\n",
    "    \n",
    "    # Map sequences to vessels (approximate based on order)\n",
    "    # This is simplified; in production, track vessel_id through sequence creation\n",
    "    sequences_per_vessel = len(X) // n_vessels\n",
    "    \n",
    "    train_mask = []\n",
    "    val_mask = []\n",
    "    test_mask = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        # Simple assignment (can be improved with proper tracking)\n",
    "        vessel_idx = min(i // max(1, sequences_per_vessel), n_vessels - 1)\n",
    "        vessel = vessels[vessel_idx]\n",
    "        \n",
    "        if vessel in train_vessels:\n",
    "            train_mask.append(i)\n",
    "        elif vessel in val_vessels:\n",
    "            val_mask.append(i)\n",
    "        else:\n",
    "            test_mask.append(i)\n",
    "    \n",
    "    X_train = X[train_mask]\n",
    "    y_train = y[train_mask]\n",
    "    X_val = X[val_mask]\n",
    "    y_val = y[val_mask]\n",
    "    X_test = X[test_mask]\n",
    "    y_test = y[test_mask]\n",
    "    \n",
    "    print(f\"Data Split (vessel-aware):\")\n",
    "    print(f\"   Train: {len(X_train):,} sequences ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Val:   {len(X_val):,} sequences ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {len(X_test):,} sequences ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    print(f\"\\n   Train - Genuine: {(y_train==0).sum()}, Spoofed: {(y_train==1).sum()}\")\n",
    "    print(f\"   Val   - Genuine: {(y_val==0).sum()}, Spoofed: {(y_val==1).sum()}\")\n",
    "    print(f\"   Test  - Genuine: {(y_test==0).sum()}, Spoofed: {(y_test==1).sum()}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Alternative: Simple random split (faster, but may have leakage)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.125, random_state=42, stratify=y_train  # 0.125 * 0.8 = 0.1 of total\n",
    ")\n",
    "\n",
    "print(f\"Data Split (random stratified):\")\n",
    "print(f\"  Train: {len(X_train):,} sequences\")\n",
    "print(f\"  Val:   {len(X_val):,} sequences\")\n",
    "print(f\"  Test:  {len(X_test):,} sequences\")\n",
    "print(f\"\\n  Train - Genuine: {(y_train==0).sum()}, Spoofed: {(y_train==1).sum()}\")\n",
    "print(f\"  Val   - Genuine: {(y_val==0).sum()}, Spoofed: {(y_val==1).sum()}\")\n",
    "print(f\"  Test  - Genuine: {(y_test==0).sum()}, Spoofed: {(y_test==1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af252ae2",
   "metadata": {},
   "source": [
    "## 4. Bi-LSTM Model Architecture\n",
    "\n",
    "PyTorch implementation of Algorithm 2 from the paper:\n",
    "- Input Layer\n",
    "- Bidirectional LSTM(62 units, return_sequences=True)\n",
    "- Bidirectional LSTM(30 units)\n",
    "- Dense(1, sigmoid)\n",
    "- Binary crossentropy loss, Adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d62c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bi-LSTM model (Algorithm 2 from paper) - PyTorch\n",
    "class BiLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch Bi-LSTM model for AIS spoofing detection.\n",
    "    \n",
    "    Architecture:\n",
    "    - Bidirectional LSTM(62, return_sequences=True)\n",
    "    - Bidirectional LSTM(30)\n",
    "    - Dense(1, sigmoid)\n",
    "    \n",
    "    Args:\n",
    "        input_size: Number of features\n",
    "        lstm_units_1: First Bi-LSTM layer units (default: 62 from paper)\n",
    "        lstm_units_2: Second Bi-LSTM layer units (default: 30 from paper)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, lstm_units_1=62, lstm_units_2=30):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        \n",
    "        # First Bidirectional LSTM (bidirectional doubles the output size)\n",
    "        self.bilstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=lstm_units_1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Second Bidirectional LSTM\n",
    "        self.bilstm2 = nn.LSTM(\n",
    "            input_size=lstm_units_1 * 2,  # Bidirectional output is doubled\n",
    "            hidden_size=lstm_units_2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(lstm_units_2 * 2, 1)  # Bidirectional output is doubled\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        # First Bi-LSTM layer\n",
    "        lstm1_out, _ = self.bilstm1(x)\n",
    "        \n",
    "        # Second Bi-LSTM layer\n",
    "        lstm2_out, _ = self.bilstm2(lstm1_out)\n",
    "        \n",
    "        # Take the last output from the second LSTM\n",
    "        last_hidden = lstm2_out[:, -1, :]\n",
    "        \n",
    "        # Dense layer + Sigmoid\n",
    "        output = self.fc(last_hidden)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def build_bilstm_model(input_size, lstm_units_1=62, lstm_units_2=30):\n",
    "    \"\"\"Create and return Bi-LSTM model.\"\"\"\n",
    "    model = BiLSTMModel(input_size, lstm_units_1, lstm_units_2)\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "# Build model\n",
    "print(\"Building Bi-LSTM Model (PyTorch)...\")\n",
    "print(f\"  Input size: {X_train.shape[2]} features\")\n",
    "\n",
    "model = build_bilstm_model(\n",
    "    input_size=X_train.shape[2],\n",
    "    lstm_units_1=62,  # From paper\n",
    "    lstm_units_2=30   # From paper\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46426fe9",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train with hyperparameters from Table 1 in paper:\n",
    "- LSTM units: 62, 30\n",
    "- Batch size: 30\n",
    "- Learning rate: 0.004\n",
    "- Epochs: 50\n",
    "- Early stopping with patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration (from paper Table 1)\n",
    "BATCH_SIZE = 30  # From paper\n",
    "EPOCHS = 50  # From paper\n",
    "LEARNING_RATE = 0.004  # From paper\n",
    "PATIENCE = 10  # Early stopping patience\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train).float(),\n",
    "    torch.from_numpy(y_train).float().reshape(-1, 1)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_val).float(),\n",
    "    torch.from_numpy(y_val).float().reshape(-1, 1)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Early stopping patience: {PATIENCE}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"\\nStarting training...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad40006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model (Algorithm 2: Custom PyTorch training loop)\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_x, batch_y in loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "        all_preds.extend(outputs.detach().cpu().numpy())\n",
    "        all_labels.extend(batch_y.detach().cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader.dataset), np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item() * batch_x.size(0)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader.dataset), np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "\n",
    "# Training loop with early stopping\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'val_accuracy': [],\n",
    "    'train_precision': [],\n",
    "    'val_precision': [],\n",
    "    'train_recall': [],\n",
    "    'val_recall': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_preds, train_labels = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_preds_binary = (train_preds > 0.6).astype(int)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "    val_preds_binary = (val_preds > 0.6).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = (train_preds_binary == train_labels).mean()\n",
    "    val_acc = (val_preds_binary == val_labels).mean()\n",
    "    \n",
    "    train_prec = precision_score(train_labels, train_preds_binary, zero_division=0)\n",
    "    val_prec = precision_score(val_labels, val_preds_binary, zero_division=0)\n",
    "    \n",
    "    train_rec = recall_score(train_labels, train_preds_binary, zero_division=0)\n",
    "    val_rec = recall_score(val_labels, val_preds_binary, zero_division=0)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_accuracy'].append(train_acc)\n",
    "    history['val_accuracy'].append(val_acc)\n",
    "    history['train_precision'].append(train_prec)\n",
    "    history['val_precision'].append(val_prec)\n",
    "    history['train_recall'].append(train_rec)\n",
    "    history['val_recall'].append(val_rec)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "\n",
    "print(f\"\\nTraining Complete!\")\n",
    "print(f\"  Best epoch: {np.argmin(history['val_loss']) + 1}\")\n",
    "print(f\"  Best val_loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"  Best val_accuracy: {max(history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Model Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision & Recall\n",
    "axes[2].plot(history['train_precision'], label='Train Precision', linewidth=2)\n",
    "axes[2].plot(history['val_precision'], label='Val Precision', linewidth=2, linestyle='--')\n",
    "axes[2].plot(history['train_recall'], label='Train Recall', linewidth=2)\n",
    "axes[2].plot(history['val_recall'], label='Val Recall', linewidth=2, linestyle='--')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Score')\n",
    "axes[2].set_title('Precision & Recall')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_root / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training plots saved to: {output_root / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65abd86b",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate on test set and compare with paper's reported metrics (Table 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set (Algorithm 3: Anomaly Score Calculation)\n",
    "print(\"Generating predictions on test set...\\n\")\n",
    "\n",
    "# Convert test data to tensors\n",
    "X_test_tensor = torch.from_numpy(X_test).float().to(device)\n",
    "y_test_tensor = torch.from_numpy(y_test).float()\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_probs = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "y_pred_probs = y_pred_probs.flatten()\n",
    "\n",
    "# Apply threshold τ = 0.6 (from paper section 3.3.2)\n",
    "THRESHOLD = 0.6  # Optimal threshold from paper\n",
    "y_pred = (y_pred_probs > THRESHOLD).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "test_loss = criterion(torch.from_numpy(y_pred_probs).float().to(device), \n",
    "                      torch.from_numpy(y_test).float().reshape(-1, 1).to(device)).item()\n",
    "test_acc = (y_pred == y_test).mean()\n",
    "\n",
    "# Additional metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Detection rate (same as recall)\n",
    "detection_rate = recall\n",
    "\n",
    "print(f\"Test Set Evaluation (threshold τ = {THRESHOLD}):\")\n",
    "print(f\"  Accuracy:       {test_acc:.4f}\")\n",
    "print(f\"  Precision:      {precision:.4f}\")\n",
    "print(f\"  Recall:         {recall:.4f}\")\n",
    "print(f\"  F1-Score:       {f1:.4f}\")\n",
    "print(f\"  Detection Rate: {detection_rate:.4f}\")\n",
    "\n",
    "# Compare with paper's Bi-LSTM results (Table 2)\n",
    "paper_results = {\n",
    "    \"Precision\": 0.94,\n",
    "    \"Recall\": 0.92,\n",
    "    \"F1-Score\": 0.93,\n",
    "    \"Detection Rate\": 0.91\n",
    "}\n",
    "\n",
    "print(f\"\\nPaper's Reported Bi-LSTM Results:\")\n",
    "for metric, value in paper_results.items():\n",
    "    print(f\"  {metric}: {value:.2f}\")\n",
    "\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"  Precision:      {'✓' if precision >= 0.85 else '✗'} (Ours: {precision:.2f}, Paper: {paper_results['Precision']:.2f})\")\n",
    "print(f\"  Recall:         {'✓' if recall >= 0.85 else '✗'} (Ours: {recall:.2f}, Paper: {paper_results['Recall']:.2f})\")\n",
    "print(f\"  F1-Score:       {'✓' if f1 >= 0.85 else '✗'} (Ours: {f1:.2f}, Paper: {paper_results['F1-Score']:.2f})\")\n",
    "print(f\"  Detection Rate: {'✓' if detection_rate >= 0.85 else '✗'} (Ours: {detection_rate:.2f}, Paper: {paper_results['Detection Rate']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "# Labels\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Genuine', 'Spoofed'])\n",
    "ax.set_yticklabels(['Genuine', 'Spoofed'])\n",
    "\n",
    "# Annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", \n",
    "                      color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n",
    "                      fontsize=20, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_root / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Genuine', 'Spoofed']))\n",
    "\n",
    "print(f\"\\nConfusion matrix saved to: {output_root / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ce7af",
   "metadata": {},
   "source": [
    "## 7. Spoofing Detection on Real Incidents\n",
    "\n",
    "Apply Algorithm 3 to detect spoofed points in actual incident trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9069339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect spoofed points in real AIS data (Algorithm 3 implementation)\n",
    "def detect_spoofing_in_data(model, df, feature_cols, scaler, sequence_length=128, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Apply trained model to detect spoofed AIS messages in real data.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with anomaly scores and predictions\n",
    "    \"\"\"\n",
    "    # Prepare features\n",
    "    df_featured, _ = extract_features(df.copy())\n",
    "    df_normalized, _ = normalize_features(df_featured, feature_cols, fit_scaler=False, scaler=scaler)\n",
    "    \n",
    "    # Create sequences per vessel\n",
    "    results = []\n",
    "    \n",
    "    for vessel_id, vessel_data in df_normalized.groupby('vessel_id'):\n",
    "        vessel_data = vessel_data.sort_values('timestamp')\n",
    "        features = vessel_data[feature_cols].values\n",
    "        \n",
    "        if len(features) < sequence_length:\n",
    "            # Skip vessels with insufficient data\n",
    "            continue\n",
    "        \n",
    "        # Create overlapping windows\n",
    "        for i in range(len(features) - sequence_length + 1):\n",
    "            seq = features[i:i+sequence_length]\n",
    "            seq = seq.reshape(1, sequence_length, len(feature_cols))\n",
    "            \n",
    "            # Predict anomaly score\n",
    "            score = model.predict(seq, verbose=0)[0][0]\n",
    "            \n",
    "            # Store result for middle point of sequence\n",
    "            mid_idx = i + sequence_length // 2\n",
    "            if mid_idx < len(vessel_data):\n",
    "                row = vessel_data.iloc[mid_idx].copy()\n",
    "                row['anomaly_score'] = score\n",
    "                row['is_spoofed'] = 1 if score > threshold else 0\n",
    "                results.append(row)\n",
    "    \n",
    "    if not results:\n",
    "        return None\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"Spoofing Detection Results:\\\")\\n    print(f\\\"  Total points analyzed: {len(results_df):,}\\\")\\n    print(f\\\"  Detected spoofed: {(results_df['is_spoofed']==1).sum():,} ({(results_df['is_spoofed']==1).sum()/len(results_df)*100:.1f}%)\\\")\\n    print(f\\\"  Mean anomaly score: {results_df['anomaly_score'].mean():.3f}\\\")\\n    print(f\\\"  Max anomaly score: {results_df['anomaly_score'].max():.3f}\\\")\\n    \\n    return results_df\\n\\n# Example: Load and test on a specific month\\nprint(\\\"Testing spoofing detection on sample data...\\\")\\ntry:\\n    # Load a test month (e.g., Sept 2017 - Agia Zoni II incident)\\n    test_data = load_month(2017, 9, data_root)\\n    print(f\\\"Loaded test data: {len(test_data):,} records\\\")\\n    \\n    # Detect spoofing\\n    incident_results = detect_spoofing_in_data(\\n        model, test_data, feature_cols, feature_scaler, \\n        sequence_length=SEQUENCE_LENGTH, threshold=THRESHOLD\\n    )\\n    \\n    if incident_results is not None:\\n        print(f\\\"\\\\nHigh-risk detections (score > 0.8):\\\")\\n        high_risk = incident_results[incident_results['anomaly_score'] > 0.8]\\n        print(f\\\"  Count: {len(high_risk)}\\\")\\n        if len(high_risk) > 0:\\n            print(high_risk[['timestamp', 'vessel_id', 'lat', 'lon', 'speed', 'anomaly_score']].head(10))\\nexcept Exception as e:\\n    print(f\\\"Could not run incident test: {e}\\\")\\n    incident_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88a7edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect spoofed points in a specific incident (Algorithm 3 implementation)\n",
    "def detect_spoofing_in_incident(incident_name, model, feature_scaler, feature_cols, \n",
    "                                 threshold=0.6, sequence_length=128):\n",
    "    \"\"\"\n",
    "    Apply spoofing detection to a specific incident.\n",
    "    \n",
    "    Returns dataframe with anomaly scores for each AIS message.\n",
    "    \"\"\"\n",
    "    # Load incident data\n",
    "    inc = next((i for i in incidents if i[\"name\"] == incident_name), None)\n",
    "    if inc is None:\n",
    "        print(f\"❌ Incident '{incident_name}' not found\")\n",
    "        return None\n",
    "    \n",
    "    slice_path = data_root / inc[\"slug\"] / \"slice.parquet\"\n",
    "    if not slice_path.exists():\n",
    "        print(f\"❌ Data not found: {slice_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load and preprocess\n",
    "    df = pd.read_parquet(slice_path)\n",
    "    df = clean_ais_data(df)\n",
    "    \n",
    "    # Extract and normalize features\n",
    "    df, _ = extract_features(df)\n",
    "    df, _ = normalize_features(df, feature_cols, fit_scaler=False, scaler=feature_scaler)\n",
    "    \n",
    "    # Create sequences per vessel\n",
    "    anomaly_scores = []\n",
    "    \n",
    "    for vessel_id, vessel_data in df.groupby('vessel_id'):\n",
    "        vessel_data = vessel_data.sort_values('timestamp')\n",
    "        features = vessel_data[feature_cols].values\n",
    "        \n",
    "        if len(features) < sequence_length:\n",
    "            # For short trajectories, use available length\n",
    "            seq = features\n",
    "            if len(seq) < 10:\n",
    "                continue\n",
    "            # Pad if needed\n",
    "            if len(seq) < sequence_length:\n",
    "                pad_length = sequence_length - len(seq)\n",
    "                seq = np.vstack([seq, np.zeros((pad_length, seq.shape[1]))])\n",
    "        else:\n",
    "            # Use sliding window\n",
    "            seq = features[:sequence_length]\n",
    "        \n",
    "        # Predict\n",
    "        seq_reshaped = seq.reshape(1, sequence_length, -1)\n",
    "        score = model.predict(seq_reshaped, verbose=0)[0][0]\n",
    "        \n",
    "        # Store scores for all points in this vessel\n",
    "        for idx in vessel_data.index:\n",
    "            anomaly_scores.append({\n",
    "                'index': idx,\n",
    "                'vessel_id': vessel_id,\n",
    "                'anomaly_score': score,\n",
    "                'is_spoofed': int(score > threshold)\n",
    "            })\n",
    "    \n",
    "    # Merge with original data\n",
    "    scores_df = pd.DataFrame(anomaly_scores)\n",
    "    df_with_scores = df.merge(scores_df, left_index=True, right_on='index', how='left')\n",
    "    \n",
    "    return df_with_scores\n",
    "\n",
    "# Test on Agia Zoni II incident\n",
    "print(\" Detecting spoofing in Agia Zoni II incident...\\n\")\n",
    "incident_results = detect_spoofing_in_incident(\n",
    "    \"Agia Zoni II\", \n",
    "    model, \n",
    "    feature_scaler, \n",
    "    feature_cols,\n",
    "    threshold=THRESHOLD\n",
    ")\n",
    "\n",
    "if incident_results is not None:\n",
    "    print(f\"✓ Detection complete for Agia Zoni II\")\n",
    "    print(f\"   Total AIS messages: {len(incident_results)}\")\n",
    "    print(f\"   Detected spoofed: {incident_results['is_spoofed'].sum()}\")\n",
    "    print(f\"   Detection rate: {incident_results['is_spoofed'].sum() / len(incident_results) * 100:.2f}%\")\n",
    "    print(f\"\\n   Anomaly score statistics:\")\n",
    "    print(f\"   Mean: {incident_results['anomaly_score'].mean():.4f}\")\n",
    "    print(f\"   Std:  {incident_results['anomaly_score'].std():.4f}\")\n",
    "    print(f\"   Max:  {incident_results['anomaly_score'].max():.4f}\")\n",
    "    print(f\"   Min:  {incident_results['anomaly_score'].min():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebec926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detected spoofing on map\n",
    "if incident_results is not None:\n",
    "    inc = next((i for i in incidents if i[\"name\"] == \"Agia Zoni II\"), None)\n",
    "    lat0, lon0 = inc[\"approx_lat\"], inc[\"approx_lon\"]\n",
    "    \n",
    "    # Create map\n",
    "    m = folium.Map(\n",
    "        location=[lat0, lon0],\n",
    "        zoom_start=12,\n",
    "        tiles=\"CartoDB positron\"\n",
    "    )\n",
    "    \n",
    "    # Feature groups\n",
    "    fg_genuine = folium.FeatureGroup(name=\"Genuine AIS\", show=True)\n",
    "    fg_spoofed = folium.FeatureGroup(name=\"Detected Spoofed\", show=True)\n",
    "    fg_incident = folium.FeatureGroup(name=\"Incident Location\", show=True)\n",
    "    \n",
    "    # Plot trajectories by vessel and spoofing status\n",
    "    for vessel_id, vessel_data in incident_results.groupby('vessel_id_x'):\n",
    "        vessel_data = vessel_data.sort_values('timestamp')\n",
    "        \n",
    "        # Split into genuine and spoofed segments\n",
    "        genuine = vessel_data[vessel_data['is_spoofed'] == 0]\n",
    "        spoofed = vessel_data[vessel_data['is_spoofed'] == 1]\n",
    "        \n",
    "        # Genuine trajectory (blue)\n",
    "        if len(genuine) > 1:\n",
    "            coords_genuine = genuine[['lat', 'lon']].values.tolist()\n",
    "            folium.PolyLine(\n",
    "                coords_genuine, \n",
    "                color='blue', \n",
    "                weight=2, \n",
    "                opacity=0.7,\n",
    "                tooltip=f\"Vessel {vessel_id} (Genuine)\"\n",
    "            ).add_to(fg_genuine)\n",
    "        \n",
    "        # Spoofed points (red)\n",
    "        if len(spoofed) > 0:\n",
    "            for idx, row in spoofed.iterrows():\n",
    "                folium.CircleMarker(\n",
    "                    location=[row['lat'], row['lon']],\n",
    "                    radius=6,\n",
    "                    color='red',\n",
    "                    fill=True,\n",
    "                    fillColor='red',\n",
    "                    fillOpacity=0.8,\n",
    "                    popup=f\"Spoofed<br>Score: {row['anomaly_score']:.3f}<br>Vessel: {vessel_id}\",\n",
    "                    tooltip=\"Detected Spoofing\"\n",
    "                ).add_to(fg_spoofed)\n",
    "    \n",
    "    # Mark incident location\n",
    "    folium.Marker(\n",
    "        location=[lat0, lon0],\n",
    "        popup=f\"<b>Agia Zoni II Incident</b><br>2017-09-10\",\n",
    "        icon=folium.Icon(color=\"red\", icon=\"exclamation\", prefix=\"fa\"),\n",
    "        tooltip=\"Incident Location\"\n",
    "    ).add_to(fg_incident)\n",
    "    \n",
    "    # Add layers\n",
    "    fg_genuine.add_to(m)\n",
    "    fg_spoofed.add_to(m)\n",
    "    fg_incident.add_to(m)\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    print(f\"  Spoofing detection map created\")\n",
    "    display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad1138",
   "metadata": {},
   "source": [
    "## 8. Model Export & Summary\n",
    "\n",
    "Save trained model, scaler, and configuration for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model artifacts\n",
    "import pickle\n",
    "\n",
    "# Save PyTorch model\n",
    "model_path = output_root / 'best_bilstm_model.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = output_root / 'feature_scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(feature_scaler, f)\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    'feature_cols': feature_cols,\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'threshold': THRESHOLD,\n",
    "    'lstm_units_1': 62,\n",
    "    'lstm_units_2': 30,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'input_size': X_train.shape[2],\n",
    "    'test_metrics': {\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'detection_rate': float(detection_rate),\n",
    "        'accuracy': float(test_acc)\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = output_root / 'model_config.pkl'\n",
    "with open(config_path, 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "\n",
    "print(f\"Model Artifacts Saved:\")\n",
    "print(f\"  Model: {model_path}\")\n",
    "print(f\"  Scaler: {scaler_path}\")\n",
    "print(f\"  Config: {config_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"Bi-LSTM SPOOFING DETECTION - PYTORCH IMPLEMENTATION COMPLETE\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"\\nFinal Test Set Performance:\")\n",
    "print(f\"  Precision:      {precision:.4f}\")\n",
    "print(f\"  Recall:         {recall:.4f}\")\n",
    "print(f\"  F1-Score:       {f1:.4f}\")\n",
    "print(f\"  Detection Rate: {detection_rate:.4f}\")\n",
    "print(f\"  Accuracy:       {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nComparison with Paper (Bi-LSTM):\")\n",
    "print(f\"  Precision:      Paper: 0.94, Ours: {precision:.2f}\")\n",
    "print(f\"  Recall:         Paper: 0.92, Ours: {recall:.2f}\")\n",
    "print(f\"  F1-Score:       Paper: 0.93, Ours: {f1:.2f}\")\n",
    "print(f\"  Detection Rate: Paper: 0.91, Ours: {detection_rate:.2f}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Apply to all 5 maritime incidents\")\n",
    "print(f\"  2. Fine-tune threshold τ for production use\")\n",
    "print(f\"  3. Integrate with TimeFM for trajectory forecasting\")\n",
    "print(f\"  4. Deploy for real-time AIS stream processing\")\n",
    "print(f\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
