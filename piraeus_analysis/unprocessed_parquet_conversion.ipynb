{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0043dbe",
   "metadata": {},
   "source": [
    "# paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851d41e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32m'107782 - The Piraeus AIS Dataset for Large-Scale Maritime Data Analytics.pdf'\u001b[0m*\n",
      " \u001b[01;32mais_augmented.parquet\u001b[0m*\n",
      " \u001b[01;32mais_cleaned.parquet\u001b[0m*\n",
      " \u001b[01;32mais_loiter.parquet\u001b[0m*\n",
      " \u001b[01;32mais_loiter_pair.parquet\u001b[0m*\n",
      " \u001b[34;42mais_static\u001b[0m/\n",
      " \u001b[34;42mgeodata\u001b[0m/\n",
      " \u001b[34;42mmodels\u001b[0m/\n",
      " \u001b[34;42mnoaa_weather\u001b[0m/\n",
      " \u001b[34;42mparquet\u001b[0m/\n",
      " \u001b[34;42mprocessed\u001b[0m/\n",
      " \u001b[34;42msar\u001b[0m/\n",
      " \u001b[34;42munipi_ais_dynamic_2017\u001b[0m/\n",
      " \u001b[34;42munipi_ais_dynamic_2018\u001b[0m/\n",
      " \u001b[34;42munipi_ais_dynamic_2019\u001b[0m/\n",
      " \u001b[34;42munipi_ais_dynamic_synopses\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ../dataset/piraeus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db762a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017  2018  2019\n"
     ]
    }
   ],
   "source": [
    "!ls ../dataset/piraeus/unipi_ais_dynamic_synopses/ais_synopses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6390edd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unipi_ais_synopses_aug_2017.csv  unipi_ais_synopses_may_2017.csv\n",
      "unipi_ais_synopses_dec_2017.csv  unipi_ais_synopses_nov_2017.csv\n",
      "unipi_ais_synopses_jul_2017.csv  unipi_ais_synopses_oct_2017.csv\n",
      "unipi_ais_synopses_jun_2017.csv  unipi_ais_synopses_sep_2017.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../dataset/piraeus/unipi_ais_dynamic_synopses/ais_synopses/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63cfd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cudf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece51b76",
   "metadata": {},
   "source": [
    "# load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70085113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d0cc199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Years:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Years:  33%|███▎      | 1/3 [01:38<03:17, 98.54s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Years:  67%|██████▋   | 2/3 [03:29<01:45, 105.67s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Years: 100%|██████████| 3/3 [05:33<00:00, 111.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_all_synopsis_csv_to_parquet(src_base: str, dst_base: str, chunksize: int = 500_000):\n",
    "    \"\"\"\n",
    "    Convert all CSVs in yearly subfolders from src_base to Parquet in dst_base,\n",
    "    with tqdm progress bars and handling nested columns.\n",
    "    \"\"\"\n",
    "    src_base = Path(src_base)\n",
    "    dst_base = Path(dst_base)\n",
    "    dst_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    schema = pa.schema([\n",
    "        pa.field(\"t\", pa.int64()),\n",
    "        pa.field(\"vessel_id\", pa.string()),\n",
    "        pa.field(\"lon\", pa.float32()),\n",
    "        pa.field(\"lat\", pa.float32()),\n",
    "        pa.field(\"heading\", pa.float32()),\n",
    "        pa.field(\"speed\", pa.float32()),\n",
    "        pa.field(\"annotations\", pa.list_(pa.string())),\n",
    "        pa.field(\"transport_trail\", pa.list_(pa.struct([\n",
    "            pa.field(\"topic\", pa.string()),\n",
    "            pa.field(\"timestamp\", pa.int64())\n",
    "        ]))),\n",
    "    ])\n",
    "\n",
    "    year_folders = [f for f in src_base.iterdir() if f.is_dir()]\n",
    "    for year_folder in tqdm(year_folders, desc=\"Years\"):\n",
    "        dst_year_folder = dst_base / year_folder.name\n",
    "        dst_year_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        csv_files = list(year_folder.glob(\"*.csv\"))\n",
    "        for csv_file in tqdm(csv_files, desc=f\"{year_folder.name} CSVs\", leave=False):\n",
    "            parquet_file = dst_year_folder / csv_file.with_suffix(\".parquet\").name\n",
    "\n",
    "            writer = pq.ParquetWriter(parquet_file, schema)\n",
    "            for chunk in tqdm(pd.read_csv(csv_file, chunksize=chunksize),\n",
    "                              desc=f\"{csv_file.name} chunks\", leave=False):\n",
    "                # Convert nested string columns to Python objects\n",
    "                chunk[\"annotations\"] = chunk[\"annotations\"].apply(eval)\n",
    "                chunk[\"transport_trail\"] = chunk[\"transport_trail\"].apply(eval)\n",
    "\n",
    "                table = pa.Table.from_pandas(chunk, schema=schema, preserve_index=False)\n",
    "                writer.write_table(table)\n",
    "            writer.close()\n",
    "convert_all_synopsis_csv_to_parquet(\n",
    "    src_base=\"../dataset/piraeus/unipi_ais_dynamic_synopses/ais_synopses\",\n",
    "    dst_base=\"../dataset/piraeus/parquet/unipi_ais_dynamic_synopses\",\n",
    "    chunksize=500_000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4da88103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017  2018  2019\n"
     ]
    }
   ],
   "source": [
    "!ls ../dataset/piraeus/parquet/unipi_ais_dynamic_synopses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d421bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unipi_ais_synopses_apr_2019.parquet  unipi_ais_synopses_jun_2019.parquet\n",
      "unipi_ais_synopses_aug_2019.parquet  unipi_ais_synopses_mar_2019.parquet\n",
      "unipi_ais_synopses_dec_2019.parquet  unipi_ais_synopses_may_2019.parquet\n",
      "unipi_ais_synopses_feb_2019.parquet  unipi_ais_synopses_nov_2019.parquet\n",
      "unipi_ais_synopses_jan_2019.parquet  unipi_ais_synopses_oct_2019.parquet\n",
      "unipi_ais_synopses_jul_2019.parquet  unipi_ais_synopses_sep_2019.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls ../dataset/piraeus/parquet/unipi_ais_dynamic_synopses/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "672b6242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.3\n",
      "21.0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas, pyarrow\n",
    "print(pandas.__version__)\n",
    "print(pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85d32f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t                                                      1554066000000\n",
      "vessel_id          1945c6f118eb4bfae9dd5ccd32cc83126542d1b5341732...\n",
      "lon                                                         23.64925\n",
      "lat                                                        37.930935\n",
      "heading                                                          NaN\n",
      "speed                                                            NaN\n",
      "annotations                                                [GAP_END]\n",
      "transport_trail    [{'topic': 'datacsv_saronikos_3', 'timestamp':...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq_file = pq.ParquetFile(\"../dataset/piraeus/parquet/unipi_ais_dynamic_synopses/2019/unipi_ais_synopses_apr_2019.parquet\")\n",
    "\n",
    "# Suppose row groups are ~500k rows each\n",
    "row_group_index = 0  # 6*500k = 3Mth row\n",
    "table = pq_file.read_row_group(row_group_index)\n",
    "\n",
    "df_chunk = table.to_pandas()  # Only this row group in memory\n",
    "row = df_chunk.iloc[0]  # Approx 3Mth row\n",
    "print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2212b5da",
   "metadata": {},
   "source": [
    "so now we can have random access in O(1) using parquet a and ready for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21ef42d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'ELF')\n",
      "3.12.12 | packaged by conda-forge | (main, Jan 26 2026, 23:51:32) [GCC 14.3.0]\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.architecture()); import sys; print(sys.version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.12-cuda13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
