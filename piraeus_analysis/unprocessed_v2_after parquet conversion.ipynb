{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0043dbe",
   "metadata": {},
   "source": [
    "# paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851d41e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107782 - The Piraeus AIS Dataset for Large-Scale Maritime Data Analytics.pdf\n",
      "ais_augmented.parquet\n",
      "ais_cleaned.parquet\n",
      "ais_loiter.parquet\n",
      "ais_loiter_pair.parquet\n",
      "ais_static\n",
      "geodata\n",
      "models\n",
      "noaa_weather\n",
      "parquet_version\n",
      "processed\n",
      "sar\n",
      "unipi_ais_dynamic_2017\n",
      "unipi_ais_dynamic_2018\n",
      "unipi_ais_dynamic_2019\n",
      "unipi_ais_dynamic_synopses\n"
     ]
    }
   ],
   "source": [
    "!ls ../dataset/piraeus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db762a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 22B1-CD8D\n",
      "\n",
      " Directory of c:\\Users\\BBBS-AI-01\\d\\anomaly\\dataset\\piraeus\\unipi_ais_dynamic_2017\n",
      "\n",
      "31/12/2025  02:53 pm    <DIR>          .\n",
      "03/02/2026  01:28 pm    <DIR>          ..\n",
      "31/12/2025  02:52 pm             1,831 README.md\n",
      "31/12/2025  02:52 pm       982,705,862 unipi_ais_dynamic_aug2017.csv\n",
      "31/12/2025  02:52 pm       734,582,968 unipi_ais_dynamic_dec2017.csv\n",
      "31/12/2025  02:52 pm     1,022,394,184 unipi_ais_dynamic_jul2017.csv\n",
      "31/12/2025  02:53 pm       993,088,856 unipi_ais_dynamic_jun2017.csv\n",
      "31/12/2025  02:53 pm       527,700,689 unipi_ais_dynamic_may2017.csv\n",
      "31/12/2025  02:53 pm       688,654,355 unipi_ais_dynamic_nov2017.csv\n",
      "31/12/2025  02:53 pm       527,318,392 unipi_ais_dynamic_oct2017.csv\n",
      "31/12/2025  02:53 pm       934,818,984 unipi_ais_dynamic_sep2017.csv\n",
      "               9 File(s)  6,411,266,121 bytes\n",
      "               2 Dir(s)  95,996,178,432 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls ..\\dataset\\piraeus\\unipi_ais_dynamic_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672b6242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.3\n",
      "23.0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas, pyarrow\n",
    "print(pandas.__version__)\n",
    "print(pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d32f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t                                                1494345047000\n",
      "vessel_id    b0b2bd45bbb8911fbea20744b0e8b98bbb0e76f6c3af37...\n",
      "lat                                                  37.929298\n",
      "lon                                                  23.682772\n",
      "heading                                                   30.0\n",
      "speed                                                      0.0\n",
      "course                                                   170.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq_file = pq.ParquetFile(\"unipi_ais_dynamic_may2017.parquet\")\n",
    "\n",
    "# Suppose row groups are ~500k rows each\n",
    "row_group_index = 6  # 6*500k = 3Mth row\n",
    "table = pq_file.read_row_group(row_group_index)\n",
    "\n",
    "df_chunk = table.to_pandas()  # Only this row group in memory\n",
    "row = df_chunk.iloc[0]  # Approx 3Mth row\n",
    "print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eebefe",
   "metadata": {},
   "source": [
    "## Random Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ab7b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 4305035, Row groups: 9\n",
      "t                                                1494345047000\n",
      "vessel_id    b0b2bd45bbb8911fbea20744b0e8b98bbb0e76f6c3af37...\n",
      "lat                                                  37.929298\n",
      "lon                                                  23.682772\n",
      "heading                                                   30.0\n",
      "speed                                                      0.0\n",
      "course                                                   170.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# Load Parquet file metadata\n",
    "pq_file = pq.ParquetFile(\"unipi_ais_dynamic_may2017.parquet\")\n",
    "num_rows = pq_file.metadata.num_rows\n",
    "num_row_groups = pq_file.num_row_groups\n",
    "\n",
    "print(f\"Total rows: {num_rows}, Row groups: {num_row_groups}\")\n",
    "\n",
    "# Function to read a row by index\n",
    "def read_row(row_idx: int) -> pd.Series:\n",
    "    if row_idx < 0 or row_idx >= num_rows:\n",
    "        raise IndexError(\"Row index out of bounds\")\n",
    "\n",
    "    cum_rows = 0\n",
    "    for group_idx in range(num_row_groups):\n",
    "        rg_rows = pq_file.metadata.row_group(group_idx).num_rows\n",
    "        if row_idx < cum_rows + rg_rows:\n",
    "            local_idx = row_idx - cum_rows\n",
    "            table = pq_file.read_row_group(group_idx)\n",
    "            df = table.to_pandas()\n",
    "            return df.iloc[local_idx]\n",
    "        cum_rows += rg_rows\n",
    "\n",
    "# Example: read 3,000,000th row\n",
    "row_3m = read_row(3_000_000)\n",
    "print(row_3m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21ef42d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'WindowsPE')\n",
      "3.10.19 | packaged by conda-forge | (main, Jan 26 2026, 23:39:36) [MSC v.1944 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.architecture()); import sys; print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55456f",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0381418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "def load_row_group(path: str, row_group_index: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single parquet row group into memory.\n",
    "    \"\"\"\n",
    "    pq_file = pq.ParquetFile(path)\n",
    "    table = pq_file.read_row_group(row_group_index)\n",
    "    return table.to_pandas()\n",
    "\n",
    "df = load_row_group(\"unipi_ais_dynamic_may2017.parquet\", 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a8f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BBBS-AI-01\\AppData\\Local\\Temp\\ipykernel_1376\\2366386778.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"timestamp\"] = pd.to_datetime(df[\"t\"], unit=\"ms\")\n"
     ]
    }
   ],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize timestamps and drop unusable rows.\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=[\"lat\", \"lon\", \"speed\", \"course\"])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"t\"], unit=\"ms\")\n",
    "    return df\n",
    "\n",
    "df = preprocess(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20bb337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_behavior_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute per-vessel behavioral features for clustering.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"vessel_id\", \"timestamp\"])\n",
    "\n",
    "    df[\"dt\"] = (\n",
    "        df.groupby(\"vessel_id\")[\"timestamp\"]\n",
    "        .diff()\n",
    "        .dt.total_seconds()\n",
    "    )\n",
    "\n",
    "    df[\"d_course\"] = (\n",
    "        df.groupby(\"vessel_id\")[\"course\"]\n",
    "        .diff()\n",
    "        .abs()\n",
    "    )\n",
    "\n",
    "    df[\"turn_rate\"] = df[\"d_course\"] / df[\"dt\"]\n",
    "\n",
    "    features = (\n",
    "        df.groupby(\"vessel_id\")\n",
    "        .agg(\n",
    "            speed_mean=(\"speed\", \"mean\"),\n",
    "            speed_std=(\"speed\", \"std\"),\n",
    "            turn_rate_mean=(\"turn_rate\", \"mean\"),\n",
    "            stop_ratio=(\"speed\", lambda x: (x < 0.5).mean()),\n",
    "            lat_mean=(\"lat\", \"mean\"),\n",
    "            lon_mean=(\"lon\", \"mean\"),\n",
    "        )\n",
    "        .fillna(0.0)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return features\n",
    "\n",
    "feat_df = compute_behavior_features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5536ac54",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 21\u001b[0m\n\u001b[0;32m     15\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspatial_cluster\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit_predict(\n\u001b[0;32m     16\u001b[0m         np\u001b[38;5;241m.\u001b[39mradians(coords)\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 21\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspatial_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m, in \u001b[0;36mspatial_clustering\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      7\u001b[0m coords \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m DBSCAN(\n\u001b[0;32m     10\u001b[0m     eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.002\u001b[39m,\n\u001b[0;32m     11\u001b[0m     min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     12\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaversine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspatial_cluster\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradians\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\rapids-23.12\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py:473\u001b[0m, in \u001b[0;36mDBSCAN.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    449\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute clusters from a data or distance matrix and predict labels.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03m        Cluster labels. Noisy samples are given the label -1.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\rapids-23.12\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\rapids-23.12\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py:421\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    419\u001b[0m neighbors_model\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# This has worst case O(n^2) memory complexity\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m neighborhoods \u001b[38;5;241m=\u001b[39m \u001b[43mneighbors_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradius_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    424\u001b[0m     n_neighbors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mlen\u001b[39m(neighbors) \u001b[38;5;28;01mfor\u001b[39;00m neighbors \u001b[38;5;129;01min\u001b[39;00m neighborhoods])\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\rapids-23.12\\lib\\site-packages\\sklearn\\neighbors\\_base.py:1277\u001b[0m, in \u001b[0;36mRadiusNeighborsMixin.radius_neighbors\u001b[1;34m(self, X, radius, return_distance, sort_results)\u001b[0m\n\u001b[0;32m   1275\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m   1276\u001b[0m delayed_query \u001b[38;5;241m=\u001b[39m delayed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree\u001b[38;5;241m.\u001b[39mquery_radius)\n\u001b[1;32m-> 1277\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_distance:\n\u001b[0;32m   1282\u001b[0m     neigh_ind, neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mchunked_results))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\rapids-23.12\\lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\rapids-23.12\\lib\\site-packages\\joblib\\parallel.py:1986\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1984\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1985\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1988\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1989\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1993\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\rapids-23.12\\lib\\site-packages\\joblib\\parallel.py:1914\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1914\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\rapids-23.12\\lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig), warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m    146\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilters \u001b[38;5;241m=\u001b[39m warning_filters\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32msklearn/neighbors/_binary_tree.pxi:1431\u001b[0m, in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree64.query_radius\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msklearn/neighbors/_binary_tree.pxi:1384\u001b[0m, in \u001b[0;36msklearn.neighbors._ball_tree.BinaryTree64.query_radius\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def spatial_clustering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DBSCAN clustering on latitude / longitude.\n",
    "    \"\"\"\n",
    "    coords = df[[\"lat\", \"lon\"]].to_numpy()\n",
    "\n",
    "    model = DBSCAN(\n",
    "        eps=0.002,\n",
    "        min_samples=50,\n",
    "        metric=\"haversine\"\n",
    "    )\n",
    "\n",
    "    df[\"spatial_cluster\"] = model.fit_predict(\n",
    "        np.radians(coords)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "df = spatial_clustering(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ef4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def behavior_clustering(features: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cluster vessels by movement behavior.\n",
    "    \"\"\"\n",
    "    X = features[\n",
    "        [\n",
    "            \"speed_mean\",\n",
    "            \"speed_std\",\n",
    "            \"turn_rate_mean\",\n",
    "            \"stop_ratio\",\n",
    "        ]\n",
    "    ].to_numpy()\n",
    "\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    model = DBSCAN(\n",
    "        eps=0.6,\n",
    "        min_samples=10\n",
    "    )\n",
    "\n",
    "    features[\"behavior_cluster\"] = model.fit_predict(X)\n",
    "    return features\n",
    "\n",
    "feat_df = behavior_clustering(feat_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
