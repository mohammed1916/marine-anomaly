{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0043dbe",
   "metadata": {},
   "source": [
    "# paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851d41e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'107782 - The Piraeus AIS Dataset for Large-Scale Maritime Data Analytics.pdf'\n",
      " ais_augmented.parquet\n",
      " ais_cleaned.parquet\n",
      " ais_loiter.parquet\n",
      " ais_loiter_pair.parquet\n",
      " ais_static\n",
      " geodata\n",
      " models\n",
      " noaa_weather\n",
      " parquet_version\n",
      " processed\n",
      " sar\n",
      " unipi_ais_dynamic_2017\n",
      " unipi_ais_dynamic_2018\n",
      " unipi_ais_dynamic_2019\n",
      " unipi_ais_dynamic_synopses\n"
     ]
    }
   ],
   "source": [
    "!ls ../dataset/piraeus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db762a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '..datasetpiraeusunipi_ais_dynamic_2017': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls ..\\dataset\\piraeus\\unipi_ais_dynamic_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "672b6242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.3\n",
      "21.0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas, pyarrow\n",
    "print(pandas.__version__)\n",
    "print(pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d32f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t                                                1494345047000\n",
      "vessel_id    b0b2bd45bbb8911fbea20744b0e8b98bbb0e76f6c3af37...\n",
      "lat                                                  37.929298\n",
      "lon                                                  23.682772\n",
      "heading                                                   30.0\n",
      "speed                                                      0.0\n",
      "course                                                   170.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq_file = pq.ParquetFile(\"unipi_ais_dynamic_may2017.parquet\")\n",
    "\n",
    "# Suppose row groups are ~500k rows each\n",
    "row_group_index = 6  # 6*500k = 3Mth row\n",
    "table = pq_file.read_row_group(row_group_index)\n",
    "\n",
    "df_chunk = table.to_pandas()  # Only this row group in memory\n",
    "row = df_chunk.iloc[0]  # Approx 3Mth row\n",
    "print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eebefe",
   "metadata": {},
   "source": [
    "## Random Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ab7b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 4305035, Row groups: 9\n",
      "t                                                1494345047000\n",
      "vessel_id    b0b2bd45bbb8911fbea20744b0e8b98bbb0e76f6c3af37...\n",
      "lat                                                  37.929298\n",
      "lon                                                  23.682772\n",
      "heading                                                   30.0\n",
      "speed                                                      0.0\n",
      "course                                                   170.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# Load Parquet file metadata\n",
    "pq_file = pq.ParquetFile(\"unipi_ais_dynamic_may2017.parquet\")\n",
    "num_rows = pq_file.metadata.num_rows\n",
    "num_row_groups = pq_file.num_row_groups\n",
    "\n",
    "print(f\"Total rows: {num_rows}, Row groups: {num_row_groups}\")\n",
    "\n",
    "# Function to read a row by index\n",
    "def read_row(row_idx: int) -> pd.Series:\n",
    "    if row_idx < 0 or row_idx >= num_rows:\n",
    "        raise IndexError(\"Row index out of bounds\")\n",
    "\n",
    "    cum_rows = 0\n",
    "    for group_idx in range(num_row_groups):\n",
    "        rg_rows = pq_file.metadata.row_group(group_idx).num_rows\n",
    "        if row_idx < cum_rows + rg_rows:\n",
    "            local_idx = row_idx - cum_rows\n",
    "            table = pq_file.read_row_group(group_idx)\n",
    "            df = table.to_pandas()\n",
    "            return df.iloc[local_idx]\n",
    "        cum_rows += rg_rows\n",
    "\n",
    "# Example: read 3,000,000th row\n",
    "row_3m = read_row(3_000_000)\n",
    "print(row_3m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21ef42d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'ELF')\n",
      "3.12.12 | packaged by conda-forge | (main, Jan 26 2026, 23:51:32) [GCC 14.3.0]\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.architecture()); import sys; print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55456f",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51a4dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-04 07:50:40.834] [CUML] [warning] Using data on device memory because knn_n_clusters = 1.\n",
      "cluster_id\n",
      "-1    499220\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unsupervised AIS clustering using cuML HDBSCAN (GPU-safe).\n",
    "\"\"\"\n",
    "\n",
    "import cudf\n",
    "from cuml.cluster import HDBSCAN\n",
    "from cuml.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Load, clean, scale, and cluster AIS data.\n",
    "    \"\"\"\n",
    "\n",
    "    df = cudf.read_parquet(\n",
    "        \"unipi_ais_dynamic_may2017.parquet\",\n",
    "        row_groups=[6]\n",
    "    )\n",
    "\n",
    "    features = df[[\"lat\", \"lon\", \"speed\"]].astype(\"float32\")\n",
    "\n",
    "    features = features.dropna()\n",
    "    # features = features[~features.isin([float(\"inf\"), float(\"-inf\")]).any(axis=1)]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    X_scaled = cudf.DataFrame(\n",
    "        X_scaled,\n",
    "        columns=[\"lat\", \"lon\", \"speed\"]\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=3,\n",
    "        min_samples=3,\n",
    "        metric=\"euclidean\"\n",
    "    )\n",
    "\n",
    "    features[\"cluster_id\"] = clusterer.fit_predict(X_scaled)\n",
    "\n",
    "    print(features[\"cluster_id\"].value_counts())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017f822",
   "metadata": {},
   "source": [
    "Across a wide range of parameters, the algorithm consistently classified all observations as noise, indicating the absence of stable density structures in the feature space. Possible that there is a need for another model or further parameter tuning. So this will be done later after further analytics on as it reuires more data to set parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dff6a7",
   "metadata": {},
   "source": [
    "The analysis was therefore redirected toward probabilistic modeling of vessel movement, where routes are represented as sequences of spatial transitions and ranked based on their empirical likelihood in the AIS data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.12-cuda13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
