{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0043dbe",
   "metadata": {},
   "source": [
    "# paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851d41e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'107782 - The Piraeus AIS Dataset for Large-Scale Maritime Data Analytics.pdf'\n",
      " ais_augmented.parquet\n",
      " ais_cleaned.parquet\n",
      " ais_loiter.parquet\n",
      " ais_loiter_pair.parquet\n",
      " ais_static\n",
      " geodata\n",
      " models\n",
      " noaa_weather\n",
      " parquet_version\n",
      " processed\n",
      " sar\n",
      " unipi_ais_dynamic_2017\n",
      " unipi_ais_dynamic_2018\n",
      " unipi_ais_dynamic_2019\n",
      " unipi_ais_dynamic_synopses\n"
     ]
    }
   ],
   "source": [
    "!ls ../dataset/piraeus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db762a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '..datasetpiraeusunipi_ais_dynamic_2017': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls ..\\dataset\\piraeus\\unipi_ais_dynamic_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "672b6242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.3\n",
      "21.0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas, pyarrow\n",
    "print(pandas.__version__)\n",
    "print(pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d32f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t                                                1494345047000\n",
      "vessel_id    b0b2bd45bbb8911fbea20744b0e8b98bbb0e76f6c3af37...\n",
      "lat                                                  37.929298\n",
      "lon                                                  23.682772\n",
      "heading                                                   30.0\n",
      "speed                                                      0.0\n",
      "course                                                   170.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq_file = pq.ParquetFile(\"unipi_ais_dynamic_may2017.parquet\")\n",
    "\n",
    "# Suppose row groups are ~500k rows each\n",
    "row_group_index = 6  # 6*500k = 3Mth row\n",
    "table = pq_file.read_row_group(row_group_index)\n",
    "\n",
    "df_chunk = table.to_pandas()  # Only this row group in memory\n",
    "row = df_chunk.iloc[0]  # Approx 3Mth row\n",
    "print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eebefe",
   "metadata": {},
   "source": [
    "## Random Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ab7b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 4305035, Row groups: 9\n",
      "t                                                1494345047000\n",
      "vessel_id    b0b2bd45bbb8911fbea20744b0e8b98bbb0e76f6c3af37...\n",
      "lat                                                  37.929298\n",
      "lon                                                  23.682772\n",
      "heading                                                   30.0\n",
      "speed                                                      0.0\n",
      "course                                                   170.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# Load Parquet file metadata\n",
    "pq_file = pq.ParquetFile(\"unipi_ais_dynamic_may2017.parquet\")\n",
    "num_rows = pq_file.metadata.num_rows\n",
    "num_row_groups = pq_file.num_row_groups\n",
    "\n",
    "print(f\"Total rows: {num_rows}, Row groups: {num_row_groups}\")\n",
    "\n",
    "# Function to read a row by index\n",
    "def read_row(row_idx: int) -> pd.Series:\n",
    "    if row_idx < 0 or row_idx >= num_rows:\n",
    "        raise IndexError(\"Row index out of bounds\")\n",
    "\n",
    "    cum_rows = 0\n",
    "    for group_idx in range(num_row_groups):\n",
    "        rg_rows = pq_file.metadata.row_group(group_idx).num_rows\n",
    "        if row_idx < cum_rows + rg_rows:\n",
    "            local_idx = row_idx - cum_rows\n",
    "            table = pq_file.read_row_group(group_idx)\n",
    "            df = table.to_pandas()\n",
    "            return df.iloc[local_idx]\n",
    "        cum_rows += rg_rows\n",
    "\n",
    "# Example: read 3,000,000th row\n",
    "row_3m = read_row(3_000_000)\n",
    "print(row_3m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21ef42d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'ELF')\n",
      "3.12.12 | packaged by conda-forge | (main, Jan 26 2026, 23:51:32) [GCC 14.3.0]\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.architecture()); import sys; print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55456f",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51a4dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-04 07:50:40.834] [CUML] [warning] Using data on device memory because knn_n_clusters = 1.\n",
      "cluster_id\n",
      "-1    499220\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unsupervised AIS clustering using cuML HDBSCAN (GPU-safe).\n",
    "\"\"\"\n",
    "\n",
    "import cudf\n",
    "from cuml.cluster import HDBSCAN\n",
    "from cuml.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Load, clean, scale, and cluster AIS data.\n",
    "    \"\"\"\n",
    "\n",
    "    df = cudf.read_parquet(\n",
    "        \"unipi_ais_dynamic_may2017.parquet\",\n",
    "        row_groups=[6]\n",
    "    )\n",
    "\n",
    "    features = df[[\"lat\", \"lon\", \"speed\"]].astype(\"float32\")\n",
    "\n",
    "    features = features.dropna()\n",
    "    # features = features[~features.isin([float(\"inf\"), float(\"-inf\")]).any(axis=1)]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    X_scaled = cudf.DataFrame(\n",
    "        X_scaled,\n",
    "        columns=[\"lat\", \"lon\", \"speed\"]\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=3,\n",
    "        min_samples=3,\n",
    "        metric=\"euclidean\"\n",
    "    )\n",
    "\n",
    "    features[\"cluster_id\"] = clusterer.fit_predict(X_scaled)\n",
    "\n",
    "    print(features[\"cluster_id\"].value_counts())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017f822",
   "metadata": {},
   "source": [
    "Across a wide range of parameters, the algorithm consistently classified all observations as noise, indicating the absence of stable density structures in the feature space. Possible that there is a need for another model or further parameter tuning. So this will be done later after further analytics on as it reuires more data to set parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dff6a7",
   "metadata": {},
   "source": [
    "The analysis was therefore redirected toward probabilistic modeling of vessel movement, where routes are represented as sequences of spatial transitions and ranked based on their empirical likelihood in the AIS data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fed8ce",
   "metadata": {},
   "source": [
    "## Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce096e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6481fe4",
   "metadata": {},
   "source": [
    "## parquet conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "150f3165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32m'107782 - The Piraeus AIS Dataset for Large-Scale Maritime Data Analytics.pdf'\u001b[0m*\n",
      " \u001b[01;32mais_augmented.parquet\u001b[0m*\n",
      " \u001b[01;32mais_cleaned.parquet\u001b[0m*\n",
      " \u001b[01;32mais_loiter.parquet\u001b[0m*\n",
      " \u001b[01;32mais_loiter_pair.parquet\u001b[0m*\n",
      " \u001b[34;42mais_static\u001b[0m/\n",
      " \u001b[34;42mgeodata\u001b[0m/\n",
      " \u001b[34;42mmodels\u001b[0m/\n",
      " \u001b[34;42mnoaa_weather\u001b[0m/\n",
      " \u001b[34;42mparquet_version\u001b[0m/\n",
      " \u001b[34;42mprocessed\u001b[0m/\n",
      " \u001b[34;42msar\u001b[0m/\n",
      " \u001b[34;42munipi_ais_dynamic_2017\u001b[0m/\n",
      " \u001b[34;42munipi_ais_dynamic_2018\u001b[0m/\n",
      " \u001b[34;42munipi_ais_dynamic_2019\u001b[0m/\n",
      " \u001b[34;42munipi_ais_dynamic_synopses\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ../dataset/piraeus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2d178c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mREADME.md\u001b[0m*                      \u001b[01;32munipi_ais_dynamic_may2017.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_aug2017.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_nov2017.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_dec2017.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_oct2017.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_jul2017.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_sep2017.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_jun2017.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls ../dataset/piraeus/unipi_ais_dynamic_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba68031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mREADME.md\u001b[0m*                      \u001b[01;32munipi_ais_dynamic_jun2018.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_apr2018.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_mar2018.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_aug2018.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_may2018.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_dec2018.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_nov2018.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_feb2018.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_oct2018.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_jan2018.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_sep2018.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_jul2018.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls ../dataset/piraeus/unipi_ais_dynamic_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa8c7267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mREADME.md\u001b[0m*                      \u001b[01;32munipi_ais_dynamic_jun2019.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_apr2019.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_mar2019.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_aug2019.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_may2019.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_dec2019.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_nov2019.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_feb2019.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_oct2019.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_jan2019.csv\u001b[0m*  \u001b[01;32munipi_ais_dynamic_sep2019.csv\u001b[0m*\n",
      "\u001b[01;32munipi_ais_dynamic_jul2019.csv\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls ../dataset/piraeus/unipi_ais_dynamic_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6176ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "schema = pa.schema([\n",
    "    pa.field(\"t\", pa.int64()),\n",
    "    pa.field(\"vessel_id\", pa.string()),\n",
    "    pa.field(\"lat\", pa.float32()),\n",
    "    pa.field(\"lon\", pa.float32()),\n",
    "    pa.field(\"heading\", pa.float32()),\n",
    "    pa.field(\"speed\", pa.float32()),\n",
    "    pa.field(\"course\", pa.float32()),\n",
    "])\n",
    "\n",
    "writer = pq.ParquetWriter(\n",
    "    \"../dataset/piraeus/parquet/unipi_ais_dynamic_aug2017.parquet\",\n",
    "    schema\n",
    ")\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    \"../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_aug2017.csv\",\n",
    "    chunksize=500_000\n",
    "):\n",
    "    table = pa.Table.from_pandas(\n",
    "        chunk,\n",
    "        schema=schema,\n",
    "        preserve_index=False\n",
    "    )\n",
    "    writer.write_table(table)\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "418a7d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_aug2017.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2017/unipi_ais_dynamic_aug2017.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_dec2017.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2017/unipi_ais_dynamic_dec2017.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_jul2017.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2017/unipi_ais_dynamic_jul2017.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_jun2017.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2017/unipi_ais_dynamic_jun2017.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_may2017.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2017/unipi_ais_dynamic_may2017.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_nov2017.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2017/unipi_ais_dynamic_nov2017.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_oct2017.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2017/unipi_ais_dynamic_oct2017.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_sep2017.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2017/unipi_ais_dynamic_sep2017.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_apr2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_apr2018.parquet\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"name 't' present in the specified schema is not found in the columns or index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 't'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pyarrow/pandas_compat.py:452\u001b[39m, in \u001b[36m_get_columns_to_convert_given_schema\u001b[39m\u001b[34m(df, schema, preserve_index)\u001b[39m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     col = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    453\u001b[39m     is_index = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 't'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pyarrow/pandas_compat.py:456\u001b[39m, in \u001b[36m_get_columns_to_convert_given_schema\u001b[39m\u001b[34m(df, schema, preserve_index)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     col = \u001b[43m_get_index_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m):\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# name not found as index level\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pyarrow/pandas_compat.py:508\u001b[39m, in \u001b[36m_get_index_level\u001b[39m\u001b[34m(df, name)\u001b[39m\n\u001b[32m    507\u001b[39m     key = \u001b[38;5;28mint\u001b[39m(name[\u001b[38;5;28mlen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__index_level_\u001b[39m\u001b[33m\"\u001b[39m):-\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pandas/core/indexes/base.py:2109\u001b[39m, in \u001b[36mIndex._get_level_values\u001b[39m\u001b[34m(self, level)\u001b[39m\n\u001b[32m   2074\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2075\u001b[39m \u001b[33;03mReturn an Index of values for requested level.\u001b[39;00m\n\u001b[32m   2076\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2107\u001b[39m \u001b[33;03mIndex(['a', 'b', 'c'], dtype='object')\u001b[39;00m\n\u001b[32m   2108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2109\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_index_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pandas/core/indexes/base.py:2019\u001b[39m, in \u001b[36mIndex._validate_index_level\u001b[39m\u001b[34m(self, level)\u001b[39m\n\u001b[32m   2018\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m level != \u001b[38;5;28mself\u001b[39m.name:\n\u001b[32m-> \u001b[39m\u001b[32m2019\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   2020\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequested level (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) does not match index name (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2021\u001b[39m     )\n",
      "\u001b[31mKeyError\u001b[39m: 'Requested level (t) does not match index name (None)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Read CSV in chunks and write to Parquet\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd.read_csv(csv_file, chunksize=\u001b[32m500_000\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     table = \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     writer.write_table(table)\n\u001b[32m     46\u001b[39m writer.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pyarrow/table.pxi:4795\u001b[39m, in \u001b[36mpyarrow.lib.Table.from_pandas\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pyarrow/pandas_compat.py:594\u001b[39m, in \u001b[36mdataframe_to_arrays\u001b[39m\u001b[34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[39m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdataframe_to_arrays\u001b[39m(df, schema, preserve_index, nthreads=\u001b[32m1\u001b[39m, columns=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    586\u001b[39m                         safe=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    587\u001b[39m     (all_names,\n\u001b[32m    588\u001b[39m      column_names,\n\u001b[32m    589\u001b[39m      column_field_names,\n\u001b[32m    590\u001b[39m      index_column_names,\n\u001b[32m    591\u001b[39m      index_descriptors,\n\u001b[32m    592\u001b[39m      index_columns,\n\u001b[32m    593\u001b[39m      columns_to_convert,\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m      convert_fields) = \u001b[43m_get_columns_to_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m     \u001b[38;5;66;03m# NOTE(wesm): If nthreads=None, then we use a heuristic to decide whether\u001b[39;00m\n\u001b[32m    598\u001b[39m     \u001b[38;5;66;03m# using a thread pool is worth it. Currently the heuristic is whether the\u001b[39;00m\n\u001b[32m    599\u001b[39m     \u001b[38;5;66;03m# nrows > 100 * ncols and ncols > 1.\u001b[39;00m\n\u001b[32m    600\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pyarrow/pandas_compat.py:379\u001b[39m, in \u001b[36m_get_columns_to_convert\u001b[39m\u001b[34m(df, schema, preserve_index, columns)\u001b[39m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    375\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mDuplicate column names found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    376\u001b[39m     )\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_columns_to_convert_given_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m column_names = []\n\u001b[32m    382\u001b[39m column_field_names = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/rapids-25.12-cuda13/lib/python3.12/site-packages/pyarrow/pandas_compat.py:459\u001b[39m, in \u001b[36m_get_columns_to_convert_given_schema\u001b[39m\u001b[34m(df, schema, preserve_index)\u001b[39m\n\u001b[32m    456\u001b[39m     col = _get_index_level(df, name)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m):\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# name not found as index level\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    460\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mname \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m present in the specified schema is not found \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min the columns or index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preserve_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    464\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mname \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m present in the specified schema corresponds \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    465\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto the index, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpreserve_index=False\u001b[39m\u001b[33m'\u001b[39m\u001b[33m was \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    466\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mspecified\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"name 't' present in the specified schema is not found in the columns or index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# Define schema\n",
    "schema = pa.schema([\n",
    "    pa.field(\"t\", pa.int64()),\n",
    "    pa.field(\"vessel_id\", pa.string()),\n",
    "    pa.field(\"lat\", pa.float32()),\n",
    "    pa.field(\"lon\", pa.float32()),\n",
    "    pa.field(\"heading\", pa.float32()),\n",
    "    pa.field(\"speed\", pa.float32()),\n",
    "    pa.field(\"course\", pa.float32()),\n",
    "])\n",
    "\n",
    "# Input and output\n",
    "input_dirs = [\n",
    "    \"../dataset/piraeus/unipi_ais_dynamic_2017\",\n",
    "    \"../dataset/piraeus/unipi_ais_dynamic_2018\",\n",
    "    \"../dataset/piraeus/unipi_ais_dynamic_2019\",\n",
    "]\n",
    "\n",
    "# Output base directory for Parquet files\n",
    "output_base_dir = \"../dataset/piraeus/parquet\"\n",
    "Path(output_base_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Loop over all CSVs in each year folder\n",
    "for input_dir in input_dirs:\n",
    "    year = Path(input_dir).name\n",
    "    year_output_dir = Path(output_base_dir) / year\n",
    "    year_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for csv_file in sorted(Path(input_dir).glob(\"*.csv\")):\n",
    "        output_file = year_output_dir / f\"{csv_file.stem}.parquet\"\n",
    "        print(f\"Processing {csv_file} -> {output_file}\")\n",
    "\n",
    "        writer = pq.ParquetWriter(output_file, schema)\n",
    "\n",
    "        # Read CSV in chunks and write to Parquet\n",
    "        for chunk in pd.read_csv(csv_file, chunksize=500_000):\n",
    "            table = pa.Table.from_pandas(chunk, schema=schema, preserve_index=False)\n",
    "            writer.write_table(table)\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "print(\"All CSVs converted to Parquet with separate files per month.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38537831",
   "metadata": {},
   "source": [
    "Due to schema mismatch, proceeding t -> timestamp and skipping files which were already processed in above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_aug2017.csv, Parquet already exists.\n",
      "Skipping ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_dec2017.csv, Parquet already exists.\n",
      "Skipping ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_jul2017.csv, Parquet already exists.\n",
      "Skipping ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_jun2017.csv, Parquet already exists.\n",
      "Skipping ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_may2017.csv, Parquet already exists.\n",
      "Skipping ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_nov2017.csv, Parquet already exists.\n",
      "Skipping ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_oct2017.csv, Parquet already exists.\n",
      "Skipping ../dataset/piraeus/unipi_ais_dynamic_2017/unipi_ais_dynamic_sep2017.csv, Parquet already exists.\n",
      "Skipping ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_apr2018.csv, Parquet already exists.\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_aug2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_aug2018.parquet\n",
      "'t' column missing in ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_aug2018.csv, falling back to 'timestamp'\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_dec2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_dec2018.parquet\n",
      "'t' column missing in ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_dec2018.csv, falling back to 'timestamp'\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_feb2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_feb2018.parquet\n",
      "'t' column missing in ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_feb2018.csv, falling back to 'timestamp'\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_jan2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_jan2018.parquet\n",
      "'t' column missing in ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_jan2018.csv, falling back to 'timestamp'\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_jul2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_jul2018.parquet\n",
      "'t' column missing in ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_jul2018.csv, falling back to 'timestamp'\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_jun2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_jun2018.parquet\n",
      "'t' column missing in ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_jun2018.csv, falling back to 'timestamp'\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_mar2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_mar2018.parquet\n",
      "'t' column missing in ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_mar2018.csv, falling back to 'timestamp'\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_may2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_may2018.parquet\n",
      "'t' column missing in ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_may2018.csv, falling back to 'timestamp'\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_nov2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_nov2018.parquet\n",
      "'t' column missing in ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_nov2018.csv, falling back to 'timestamp'\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_oct2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_oct2018.parquet\n",
      "'t' column missing in ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_oct2018.csv, falling back to 'timestamp'\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_sep2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_sep2018.parquet\n",
      "'t' column missing in ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_sep2018.csv, falling back to 'timestamp'\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_apr2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_apr2019.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_aug2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_aug2019.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_dec2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_dec2019.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_feb2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_feb2019.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_jan2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_jan2019.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_jul2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_jul2019.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_jun2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_jun2019.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_mar2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_mar2019.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_may2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_may2019.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_nov2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_nov2019.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_oct2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_oct2019.parquet\n",
      "Processing ../dataset/piraeus/unipi_ais_dynamic_2019/unipi_ais_dynamic_sep2019.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2019/unipi_ais_dynamic_sep2019.parquet\n",
      "All CSVs converted to Parquet (skipped existing, fallback applied).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "# Input and output\n",
    "input_dirs = [\n",
    "    \"../dataset/piraeus/unipi_ais_dynamic_2017\",\n",
    "    \"../dataset/piraeus/unipi_ais_dynamic_2018\",\n",
    "    \"../dataset/piraeus/unipi_ais_dynamic_2019\",\n",
    "]\n",
    "\n",
    "output_base_dir = Path(\"../dataset/piraeus/parquet\")\n",
    "output_base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Loop over all CSVs in each year folder\n",
    "for input_dir in input_dirs:\n",
    "    year = Path(input_dir).name\n",
    "    year_output_dir = output_base_dir / year\n",
    "    year_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for csv_file in sorted(Path(input_dir).glob(\"*.csv\")):\n",
    "        output_file = year_output_dir / f\"{csv_file.stem}.parquet\"\n",
    "\n",
    "        if output_file.exists():\n",
    "            print(f\"Skipping {csv_file}, Parquet already exists.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {csv_file} -> {output_file}\")\n",
    "\n",
    "        # Try schema with 't', fallback to 'timestamp'\n",
    "        try:\n",
    "            schema = pa.schema([\n",
    "                pa.field(\"t\", pa.int64()),\n",
    "                pa.field(\"vessel_id\", pa.string()),\n",
    "                pa.field(\"lat\", pa.float32()),\n",
    "                pa.field(\"lon\", pa.float32()),\n",
    "                pa.field(\"heading\", pa.float32()),\n",
    "                pa.field(\"speed\", pa.float32()),\n",
    "                pa.field(\"course\", pa.float32()),\n",
    "            ])\n",
    "\n",
    "            writer = pq.ParquetWriter(output_file, schema)\n",
    "            for chunk in pd.read_csv(csv_file, chunksize=500_000):\n",
    "                table = pa.Table.from_pandas(chunk, schema=schema, preserve_index=False)\n",
    "                writer.write_table(table)\n",
    "            writer.close()\n",
    "\n",
    "        except KeyError as e:\n",
    "            if \"'t'\" in str(e):\n",
    "                print(f\"'t' column missing in {csv_file}, falling back to 'timestamp'\")\n",
    "                schema = pa.schema([\n",
    "                    pa.field(\"timestamp\", pa.int64()),\n",
    "                    pa.field(\"vessel_id\", pa.string()),\n",
    "                    pa.field(\"lat\", pa.float32()),\n",
    "                    pa.field(\"lon\", pa.float32()),\n",
    "                    pa.field(\"heading\", pa.float32()),\n",
    "                    pa.field(\"speed\", pa.float32()),\n",
    "                    pa.field(\"course\", pa.float32()),\n",
    "                ])\n",
    "\n",
    "                writer = pq.ParquetWriter(output_file, schema)\n",
    "                for chunk in pd.read_csv(csv_file, chunksize=500_000):\n",
    "                    # rename 'timestamp' to match schema if needed\n",
    "                    if \"t\" not in chunk.columns and \"timestamp\" in chunk.columns:\n",
    "                        chunk = chunk.rename(columns={\"timestamp\": \"timestamp\"})\n",
    "                    table = pa.Table.from_pandas(chunk, schema=schema, preserve_index=False)\n",
    "                    writer.write_table(table)\n",
    "                writer.close()\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "print(\"All CSVs converted to Parquet (skipped existing, fallback applied).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190d5fb",
   "metadata": {},
   "source": [
    "reprocess unipi_ais_dynamic_apr2018.csv so that it might not be currupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59add174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'t' column missing, falling back to 'timestamp'\n",
      "Reprocessed ../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_apr2018.csv -> ../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_apr2018.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "csv_file = Path(\"../dataset/piraeus/unipi_ais_dynamic_2018/unipi_ais_dynamic_apr2018.csv\")\n",
    "output_file = Path(\"../dataset/piraeus/parquet/unipi_ais_dynamic_2018/unipi_ais_dynamic_apr2018.parquet\")\n",
    "output_file.parent.mkdir(exist_ok=True)\n",
    "\n",
    "# Try schema with 't', fallback to 'timestamp'\n",
    "try:\n",
    "    schema = pa.schema([\n",
    "        pa.field(\"t\", pa.int64()),\n",
    "        pa.field(\"vessel_id\", pa.string()),\n",
    "        pa.field(\"lat\", pa.float32()),\n",
    "        pa.field(\"lon\", pa.float32()),\n",
    "        pa.field(\"heading\", pa.float32()),\n",
    "        pa.field(\"speed\", pa.float32()),\n",
    "        pa.field(\"course\", pa.float32()),\n",
    "    ])\n",
    "\n",
    "    writer = pq.ParquetWriter(output_file, schema)\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=500_000):\n",
    "        table = pa.Table.from_pandas(chunk, schema=schema, preserve_index=False)\n",
    "        writer.write_table(table)\n",
    "    writer.close()\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"'t' column missing, falling back to 'timestamp'\")\n",
    "    schema = pa.schema([\n",
    "        pa.field(\"timestamp\", pa.int64()),\n",
    "        pa.field(\"vessel_id\", pa.string()),\n",
    "        pa.field(\"lat\", pa.float32()),\n",
    "        pa.field(\"lon\", pa.float32()),\n",
    "        pa.field(\"heading\", pa.float32()),\n",
    "        pa.field(\"speed\", pa.float32()),\n",
    "        pa.field(\"course\", pa.float32()),\n",
    "    ])\n",
    "\n",
    "    writer = pq.ParquetWriter(output_file, schema)\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=500_000):\n",
    "        if \"t\" not in chunk.columns and \"timestamp\" in chunk.columns:\n",
    "            chunk = chunk.rename(columns={\"timestamp\": \"timestamp\"})\n",
    "        table = pa.Table.from_pandas(chunk, schema=schema, preserve_index=False)\n",
    "        writer.write_table(table)\n",
    "    writer.close()\n",
    "\n",
    "print(f\"Reprocessed {csv_file} -> {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.12-cuda13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
